{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from glob import glob\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just assume fixed CV size for ensemble with evaluation\n",
    "cvSize = 5\n",
    "numClasses = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First argument is folder, filled with CV results files\n",
    "all_preds_path = '/content/clouderizer/melanoma/out/2020.test_effb0_rr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second argument indicates, whether we are only generating predictions or actually evaluating performance on something\n",
    "if 'eval' in sys.argv[2]:\n",
    "    evaluate = True\n",
    "    # Determin if vote or average is used\n",
    "    if 'vote' in sys.argv[2]:\n",
    "        evaluate_method = 'vote'\n",
    "    else:\n",
    "        evaluate_method = 'average'\n",
    "    # Determine if exhaustive combination search or ordered search is used\n",
    "    if 'exhaust' in sys.argv[2]:\n",
    "        exhaustive_search = True\n",
    "        num_top_models = [int(s) for s in re.findall(r'\\d+',sys.argv[2])][-1]\n",
    "    else:\n",
    "        exhaustive_search = False\n",
    "    # Third argument indicates where subset should be saved\n",
    "    if 'subSet' in sys.argv[3]:\n",
    "        subSetPath = sys.argv[3]\n",
    "    else:\n",
    "        subSetPath = None\n",
    "else:\n",
    "    evaluate = False\n",
    "    acceptedList = []\n",
    "    if 'last' in sys.argv[2]:\n",
    "        acceptedList.append('last')\n",
    "    if 'best' in sys.argv[2]:\n",
    "        acceptedList.append('best')\n",
    "    if 'meta' in sys.argv[2]:\n",
    "        acceptedList.append('meta')                \n",
    "    # Third argument indicates whether some subset should be used\n",
    "    if 'subSet' in sys.argv[3]:\n",
    "        # Load subset file\n",
    "        with open(sys.argv[3],'rb') as f:\n",
    "            subSetDict = pickle.load(f)       \n",
    "        subSet = subSetDict['subSet']\n",
    "    else:\n",
    "        subSet = None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Fourth argument indicates csv path to save final results into\n",
    "if len(sys.argv) > 4 and 'csvFile' in sys.argv[4]:\n",
    "    csvPath = sys.argv[4]\n",
    "    origFilePath = sys.argv[5]\n",
    "else:\n",
    "    csvPath = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to get some metrics back\n",
    "def get_metrics(predictions,targets):\n",
    "    # Calculate metrics\n",
    "    # Accuarcy\n",
    "    acc = np.mean(np.equal(np.argmax(predictions,1),np.argmax(targets,1)))\n",
    "    # Confusion matrix\n",
    "    conf = confusion_matrix(np.argmax(targets,1),np.argmax(predictions,1))     \n",
    "    # Class weighted accuracy\n",
    "    wacc = conf.diagonal()/conf.sum(axis=1)  \n",
    "    # Auc\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = np.zeros([numClasses])\n",
    "    for i in range(numClasses):\n",
    "        fpr[i], tpr[i], _ = roc_curve(targets[:, i], predictions[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])       \n",
    "    # F1 Score\n",
    "    f1 = f1_score(np.argmax(predictions,1),np.argmax(targets,1),average='weighted')        \n",
    "    # Print\n",
    "    print(\"Accuracy:\",acc)\n",
    "    print(\"F1-Score:\",f1)\n",
    "    print(\"WACC:\",wacc)\n",
    "    print(\"Mean WACC:\",np.mean(wacc))\n",
    "    print(\"AUC:\",roc_auc)\n",
    "    print(\"Mean Auc:\",np.mean(roc_auc))        \n",
    "    return acc, f1, wacc, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If its actual evaluation, evaluate each CV indipendently, show results both for each CV set and all of them together\n",
    "if evaluate:\n",
    "    # Go through all files\n",
    "    files = sorted(glob(all_preds_path+'/*'))\n",
    "    # Because of unkown prediction size, dont use matrix\n",
    "    final_preds = {}\n",
    "    final_targets = {}\n",
    "    all_waccs = []\n",
    "    accum_preds = {}\n",
    "    # Define each pred size in loop\n",
    "    firstLoaded = False\n",
    "    for j in range(len(files)):\n",
    "        # Skip if it is a directory\n",
    "        if os.path.isdir(files[j]):\n",
    "            continue\n",
    "        # Skip if not a pkl file\n",
    "        if '.pkl' not in files[j]:\n",
    "            print(\"Remove non-pkl files\")\n",
    "            break\n",
    "        # Load file\n",
    "        with open(files[j],'rb') as f:\n",
    "            allDataCurr = pickle.load(f)    \n",
    "        # Get predictions\n",
    "        if not firstLoaded:\n",
    "            # Define accumulated prediction size\n",
    "            for i in range(cvSize):\n",
    "                accum_preds[i] = np.zeros([len(files),len(allDataCurr['bestPred'][i]),numClasses])\n",
    "            firstLoaded = True\n",
    "        # Write preds into array\n",
    "        #print(files[j],allDataCurr['bestPred'][0].shape)\n",
    "        wacc_avg = 0\n",
    "        for i in range(cvSize):\n",
    "            accum_preds[i][j,:,:] = allDataCurr['bestPred'][i]\n",
    "            final_targets[i] = allDataCurr['targets'][i]\n",
    "            # Confusion matrix\n",
    "            conf = confusion_matrix(np.argmax(allDataCurr['targets'][i],1),np.argmax(allDataCurr['bestPred'][i],1))     \n",
    "            # Class weighted accuracy\n",
    "            wacc_avg += np.mean(conf.diagonal()/conf.sum(axis=1))  \n",
    "        wacc_avg = wacc_avg/cvSize    \n",
    "        all_waccs.append(wacc_avg)         \n",
    "        # Print performance of model + name\n",
    "        print(\"Model:\",files[j],\"WACC:\",wacc_avg)\n",
    "    # Print results per cv\n",
    "    # Averaging predictions\n",
    "    f1_avg = 0\n",
    "    acc_avg = 0\n",
    "    auc_avg = np.zeros([numClasses])\n",
    "    wacc_avg = np.zeros([numClasses])\n",
    "    # Voting with predictions\n",
    "    f1_vote = 0\n",
    "    acc_vote = 0\n",
    "    auc_vote = np.zeros([numClasses])\n",
    "    wacc_vote = np.zeros([numClasses])\n",
    "    # Linear SVM on predictions\n",
    "    f1_linsvm = 0\n",
    "    acc_linsvm = 0\n",
    "    auc_linsvm = np.zeros([numClasses])\n",
    "    wacc_linsvm = np.zeros([numClasses])\n",
    "    # RF on predictions\n",
    "    f1_rf = 0\n",
    "    acc_rf = 0\n",
    "    auf_rf = np.zeros([numClasses])\n",
    "    wacc_rf = np.zeros([numClasses])\n",
    "    # Helper function to determine top combination\n",
    "    def evalEnsemble(currComb,eval_auc=False):\n",
    "        currWacc = np.zeros([cvSize])\n",
    "        currAUC = np.zeros([cvSize])\n",
    "        for i in range(cvSize):\n",
    "            if evaluate_method == 'vote':\n",
    "                pred_argmax = np.argmax(accum_preds[i][currComb,:,:],2)   \n",
    "                pred_eval = np.zeros([pred_argmax.shape[1],numClasses]) \n",
    "                for j in range(pred_eval.shape[0]):\n",
    "                    pred_eval[j,:] = np.bincount(pred_argmax[:,j],minlength=numClasses)  \n",
    "            else:\n",
    "                pred_eval = np.mean(accum_preds[i][currComb,:,:],0)\n",
    "            # Confusion matrix\n",
    "            conf = confusion_matrix(np.argmax(final_targets[i],1),np.argmax(pred_eval,1))     \n",
    "            # Class weighted accuracy\n",
    "            currWacc[i] = np.mean(conf.diagonal()/conf.sum(axis=1))   \n",
    "            if eval_auc:\n",
    "                currAUC_ = np.zeros([numClasses])\n",
    "                for j in range(numClasses):\n",
    "                    fpr, tpr, _ = roc_curve(final_targets[i][:,j], pred_eval[:, j])\n",
    "                    currAUC_[j] = auc(fpr, tpr)\n",
    "                currAUC[i] = np.mean(currAUC_)                \n",
    "        if eval_auc:\n",
    "            currAUCstd = np.std(currAUC)\n",
    "            currAUC = np.mean(currAUC)\n",
    "        else:\n",
    "            currAUCstd = currAUC\n",
    "        currWaccStd = np.std(currWacc)\n",
    "        currWacc = np.mean(currWacc)\n",
    "        if eval_auc:\n",
    "            return currWacc, currWaccStd, currAUC, currAUCstd       \n",
    "        else:\n",
    "            return currWacc\n",
    "    if exhaustive_search:\n",
    "        # First: determine best subset based on average CV wacc\n",
    "        # Select best subset based on wacc metric\n",
    "        # Only take top N models\n",
    "        top_inds = np.argsort(-np.array(all_waccs))\n",
    "        elements = top_inds[:num_top_models]\n",
    "        allCombs = []\n",
    "        for L in range(0, len(elements)+1):\n",
    "            for subset in itertools.combinations(elements, L):\n",
    "                allCombs.append(subset)\n",
    "                #print(subset)\n",
    "        print(\"Number of combinations\",len(allCombs))\n",
    "        print(\"Models considered\")\n",
    "        for i in range(len(elements)):\n",
    "            print(\"ID\",elements[i],files[elements[i]]) \n",
    "        #allWaccs = np.zeros([len(allCombs)])\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(\"Cores available\",num_cores)\n",
    "        allWaccs = Parallel(n_jobs=num_cores)(delayed(evalEnsemble)(comb) for comb in allCombs)\n",
    "        # Sort by highest value\n",
    "        allWaccsSrt = -np.sort(-np.array(allWaccs))\n",
    "        srtInds = np.argsort(-np.array(allWaccs))\n",
    "        allCombsSrt = np.array(allCombs)[srtInds]\n",
    "        for i in range(5):\n",
    "            print(\"Top\",i+1)\n",
    "            print(\"Best WACC\",allWaccsSrt[i])       \n",
    "            wacc, wacc_std, auc_val, auc_val_std = evalEnsemble(allCombsSrt[i],eval_auc=True)\n",
    "            print(\"Metrics WACC %.4f +- %.4f AUC %.4f +- %.4f\"%(wacc,wacc_std,auc_val,auc_val_std))     \n",
    "            print(\"Best Combination:\",allCombsSrt[i])\n",
    "            print(\"Corresponding File Names\")\n",
    "            subSetDict = {}\n",
    "            subSetDict['subSet'] = []\n",
    "            for j in allCombsSrt[i]:\n",
    "                print(\"ID\",j,files[j])  \n",
    "                # Add filename without last part, indicating the type \"best/last/meta/full\"\n",
    "                if i == 0:                \n",
    "                    subSetDict['subSet'].append(files[j])    \n",
    "            print(\"---------------------------------------------\")                 \n",
    "        bestComb = allCombsSrt[0]     \n",
    "    else:\n",
    "        # Only take top N models\n",
    "        top_inds = np.argsort(-np.array(all_waccs))\n",
    "        # Go through all top N combs\n",
    "        allWaccs = np.zeros([len(top_inds)])\n",
    "        allCombs = []\n",
    "        for i in range(len(top_inds)):\n",
    "            allCombs.append([])\n",
    "            if i==0:\n",
    "                allCombs[i].append(top_inds[0])\n",
    "            else:\n",
    "                allCombs[i] = copy.deepcopy(allCombs[i-1])\n",
    "                allCombs[i].append(top_inds[i])\n",
    "            # Test comb\n",
    "            allWaccs[i] = evalEnsemble(allCombs[i])\n",
    "        # Sort by highest value\n",
    "        allWaccsSrt = -np.sort(-np.array(allWaccs))\n",
    "        srtInds = np.argsort(-np.array(allWaccs))\n",
    "        allCombsSrt = np.array(allCombs)[srtInds]\n",
    "        for i in range(len(top_inds)):\n",
    "            print(\"Top\",i+1)\n",
    "            print(\"WACC\",allWaccsSrt[i])  \n",
    "            wacc, wacc_std, auc_val, auc_val_std = evalEnsemble(allCombsSrt[i],eval_auc=True)\n",
    "            print(\"Metrics WACC %.4f +- %.4f AUC %.4f +- %.4f\"%(wacc,wacc_std,auc_val,auc_val_std))           \n",
    "            print(\"Combination:\",allCombsSrt[i])\n",
    "            if i == 0:\n",
    "                subSetDict = {}\n",
    "                subSetDict['subSet'] = []\n",
    "                for j in allCombsSrt[i]:\n",
    "                    print(\"ID\",j,files[j])  \n",
    "                    # Add filename without last part, indicating the type \"best/last/meta/full\"\n",
    "                    subSetDict['subSet'].append(files[j])\n",
    "            print(\"---------------------------------------------\") \n",
    "        p#rint(\"Corresponding File Names\")  \n",
    "        #for j in allCombs[-1]:\n",
    "        #    print(\"ID\",j,files[j])                          \n",
    "        bestComb = allCombsSrt[0]    \n",
    "    # Save subset for later\n",
    "    if subSetPath is not None:\n",
    "        with open(subSetPath, 'wb') as f:\n",
    "            pickle.dump(subSetDict, f, pickle.HIGHEST_PROTOCOL)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    # Only generate predictions. All models predict on the same set -> cv models are equal to full models here    \n",
    "    # Go through all files\n",
    "    files = sorted(glob(all_preds_path+'/*'))\n",
    "    # Because of unkown prediction size, only determin it in the loop\n",
    "    firstLoaded = False\n",
    "    ind = 0\n",
    "    for j in range(len(files)):\n",
    "        # Skip if not a pkl file\n",
    "        if '.pkl' not in files[j]:\n",
    "            continue\n",
    "        # Potentially check, if this file is among the selected subset\n",
    "        if subSet is not None:\n",
    "            # Search\n",
    "            found = False\n",
    "            for name in subSet:\n",
    "                _, name_only = name.split('ISIC')\n",
    "                if name_only in files[j]:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                # Check extra for acceptedList inclusion\n",
    "                for name in subSet:\n",
    "                    _, name_only = name.split('ISIC')\n",
    "                    if name_only[:-13] in files[j]:\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    continue\n",
    "                # Then check, whether this type of \"best,last,meta,full\" is desired\n",
    "                found = False\n",
    "                for name in acceptedList:\n",
    "                    if name in files[j]:\n",
    "                        found = True\n",
    "                        break\n",
    "                if not found:\n",
    "                    continue            \n",
    "        # Load file\n",
    "        with open(files[j],'rb') as f:\n",
    "            allDataCurr = pickle.load(f)    \n",
    "        # Get predictions\n",
    "        if not firstLoaded:\n",
    "            # Define final prediction/targets size, assume fixed CV size\n",
    "            final_preds = np.zeros([len(allDataCurr['extPred'][0]),numClasses])\n",
    "            # Define accumulated prediction size\n",
    "            accum_preds = np.expand_dims(allDataCurr['extPred'][0],0)\n",
    "            ind += 1\n",
    "            if len(allDataCurr['extPred']) > 1:\n",
    "                for i in range(1,len(allDataCurr['extPred'])):\n",
    "                    accum_preds = np.concatenate((accum_preds,np.expand_dims(allDataCurr['extPred'][i],0)),0)\n",
    "                    ind += 1\n",
    "            else:\n",
    "                # Just repeat the first model X times\n",
    "                for i in range(1,5):\n",
    "                    accum_preds = np.concatenate((accum_preds,np.expand_dims(allDataCurr['extPred'][0],0)),0)\n",
    "                    ind += 1                \n",
    "            firstLoaded = True\n",
    "        else:\n",
    "            # Write preds into array\n",
    "            if len(allDataCurr['extPred']) > 1:\n",
    "                for i in range(len(allDataCurr['extPred'])):\n",
    "                    accum_preds = np.concatenate((accum_preds,np.expand_dims(allDataCurr['extPred'][i],0)),0)\n",
    "                    ind += 1\n",
    "            else:\n",
    "                # Just repeat the first model X times\n",
    "                for i in range(0,5):\n",
    "                    accum_preds = np.concatenate((accum_preds,np.expand_dims(allDataCurr['extPred'][0],0)),0)\n",
    "                    ind += 1                       \n",
    "        print(files[j])\n",
    "    # Resize array to actually used size\n",
    "    print(accum_preds.shape)\n",
    "    final_preds = accum_preds[:ind,:,:]\n",
    "    print(final_preds.shape)\n",
    "    # Average for final predictions\n",
    "    final_preds = np.mean(final_preds,0)\n",
    "    class_pred = np.argmax(final_preds,1)\n",
    "    print(np.mean(final_preds,0))\n",
    "    # Write into csv file, according to ordered list\n",
    "    if csvPath is not None:\n",
    "        # Get order file names from original folder\n",
    "        files = sorted(glob(origFilePath+'/*'))\n",
    "        # save into formatted csv file\n",
    "        with open(csvPath, 'w') as csv_file:\n",
    "            # First line\n",
    "            csv_file.write(\"image,MEL,NV,BCC,AK,BKL,DF,VASC,SCC,UNK\\n\")\n",
    "            ind = 0\n",
    "            for file_name in files:\n",
    "                if 'ISIC_' not in file_name:\n",
    "                    continue\n",
    "                splits = file_name.split('\\\\')\n",
    "                name = splits[-1]\n",
    "                name, _ = name.split('.')\n",
    "                csv_file.write(name + \",\" + str(final_preds[ind,0]) + \",\" +  str(final_preds[ind,1]) + \",\" + str(final_preds[ind,2]) + \",\" + str(final_preds[ind,3]) + \",\" + str(final_preds[ind,4]) + \",\" + str(final_preds[ind,5]) + \",\" + str(final_preds[ind,6]) + \",\" + str(final_preds[ind,7]) + \",\" + str(final_preds[ind,8]) + \"\\n\")\n",
    "                ind += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

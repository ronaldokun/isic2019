{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models as tv_models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import threading\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import re\n",
    "import gc\n",
    "import importlib\n",
    "import time\n",
    "import sklearn.preprocessing\n",
    "import utils\n",
    "from sklearn.utils import class_weight\n",
    "import psutil\n",
    "import models\n",
    "from cfgs.test_effb0_rr_meta import *\n",
    "from fastprogress import progress_bar as tqdm\n",
    "#from tqdm import tqdm\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add configuration file\n",
    "# Dictionary for model configuration\n",
    "arg1 = 'config'\n",
    "arg2 = 'test_effb0_rr_meta'\n",
    "arg3 = 'gpu0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import machine config\n",
    "#pc_cfg = importlib.import_module('pc_cfgs.'+arg1)\n",
    "#mdlParams.update(pc_cfg.mdlParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model config\n",
    "model_cfg = importlib.import_module('cfgs.'+arg2)\n",
    "mdlParams_model = params #model_cfg.init(mdlParams)\n",
    "mdlParams.update(mdlParams_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate training\n",
    "mdlParams['trainSetState'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path name from filename\n",
    "mdlParams['saveDirBase'] = mdlParams['save_dir'] / arg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices to use: 0\n"
     ]
    }
   ],
   "source": [
    "# Set visible devices\n",
    "if 'gpu' in arg3:\n",
    "    mdlParams['numGPUs']= [[int(s) for s in re.findall(r'\\d+',arg3)][-1]]\n",
    "    cuda_str = \"\"\n",
    "    for i in range(len(mdlParams['numGPUs'])):\n",
    "        cuda_str = cuda_str + str(mdlParams['numGPUs'][i])\n",
    "        if i is not len(mdlParams['numGPUs'])-1:\n",
    "            cuda_str = cuda_str + \",\"\n",
    "    print(\"Devices to use:\",cuda_str)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_str      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify val set to train for\n",
    "if len(sys.argv) > 4:\n",
    "    mdlParams['cv_subset'] = [int(s) for s in re.findall(r'\\d+',sys.argv[4])]\n",
    "    print(\"Training validation sets\",mdlParams['cv_subset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set during training.\n"
     ]
    }
   ],
   "source": [
    "# Check if there is a validation set, if not, evaluate train error instead\n",
    "if 'valIndCV' in mdlParams or 'valInd' in mdlParams:\n",
    "    eval_set = 'valInd'\n",
    "    print(\"Evaluating on validation set during training.\")\n",
    "else:\n",
    "    eval_set = 'trainInd'\n",
    "    print(\"No validation set, evaluating on training set during training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/clouderizer/melanoma/out/test_effb0_rr_meta')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlParams['saveDirBase']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there were previous ones that have alreary bin learned\n",
    "prevFile = Path(mdlParams['saveDirBase'] / '/CV.pkl')\n",
    "#print(prevFile)\n",
    "if prevFile.exists():\n",
    "    print(\"Part of CV already done\")\n",
    "    with open(mdlParams['saveDirBase'] / '/CV.pkl', 'rb') as f:\n",
    "        allData = pickle.load(f)\n",
    "else:\n",
    "    allData = {}\n",
    "    allData['f1Best'] = {}\n",
    "    allData['sensBest'] = {}\n",
    "    allData['specBest'] = {}\n",
    "    allData['accBest'] = {}\n",
    "    allData['waccBest'] = {}\n",
    "    allData['aucBest'] = {}\n",
    "    allData['convergeTime'] = {}\n",
    "    allData['bestPred'] = {}\n",
    "    allData['targets'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take care of CV\n",
    "if mdlParams.get('cv_subset',None) is not None:\n",
    "    cv_set = mdlParams['cv_subset']\n",
    "else:\n",
    "    cv_set = iter(range(mdlParams['numCV']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV set 3\n",
      "cuda:0\n",
      "Current class weights [0.54936356 5.56446503]\n",
      "Current class weights with extra [0.54936356 5.56446503]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'meta_array'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b55a858eaf2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;31m# Meta scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdlParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meta_features'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scale_features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_scaler_meta'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'meta_array'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainInd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_scaler_meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"var\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmdlParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_scaler_meta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'meta_array'"
     ]
    }
   ],
   "source": [
    "for cv in cv_set:\n",
    "\n",
    "    # Check if this fold was already trained\n",
    "    already_trained = False\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['saveDir'] = mdlParams['saveDirBase'] / f'CVSet{str(cv)}'\n",
    "        if os.path.isdir(mdlParams['saveDirBase']):\n",
    "            if os.path.isdir(mdlParams['saveDir']):\n",
    "                all_max_iter = []\n",
    "                for name in os.listdir(mdlParams['saveDir']):\n",
    "                    int_list = [int(s) for s in re.findall(r'\\d+',name)]\n",
    "                    if len(int_list) > 0:\n",
    "                        all_max_iter.append(int_list[-1])\n",
    "                    #if '-' + str(mdlParams['training_steps'])+ '.pt' in name:\n",
    "                    #    print(\"Fold %d already fully trained\"%(cv))\n",
    "                    #    already_trained = True\n",
    "                all_max_iter = np.array(all_max_iter)\n",
    "                if len(all_max_iter) > 0 and np.max(all_max_iter) >= mdlParams['training_steps']:\n",
    "                    print(\"Fold %d already fully trained with %d iterations\"%(cv,np.max(all_max_iter)))\n",
    "                    already_trained = True\n",
    "\n",
    "    if already_trained:\n",
    "        #continue\n",
    "        pass\n",
    "    print(\"CV set\",cv)\n",
    "    # Reset model graph \n",
    "    importlib.reload(models)\n",
    "    #importlib.reload(torchvision)\n",
    "    # Collect model variables\n",
    "    modelVars = {}\n",
    "    #print(\"here\")\n",
    "    modelVars['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(modelVars['device'])\n",
    "    # Def current CV set\n",
    "    mdlParams['trainInd'] = mdlParams['trainIndCV'][cv]\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['valInd'] = mdlParams['valIndCV'][cv]\n",
    "    # Def current path for saving stuff\n",
    "    if 'valIndCV' in mdlParams:\n",
    "        mdlParams['saveDir'] = mdlParams['saveDirBase'] / f'CVSet{str(cv)}' \n",
    "    else:\n",
    "        mdlParams['saveDir'] = mdlParams['saveDirBase']\n",
    "    # Create basepath if it doesnt exist yet\n",
    "    if not os.path.isdir(mdlParams['saveDirBase']):\n",
    "        os.mkdir(mdlParams['saveDirBase'])\n",
    "    # Check if there is something to load\n",
    "    load_old = 0\n",
    "    if os.path.isdir(mdlParams['saveDir']):\n",
    "        # Check if a checkpoint is in there\n",
    "        if len([name for name in os.listdir(mdlParams['saveDir'])]) > 0:\n",
    "            load_old = 1\n",
    "            print(\"Loading old model\")\n",
    "        else:\n",
    "            # Delete whatever is in there (nothing happens)\n",
    "            filelist = [os.remove(mdlParams['saveDir'] +'/'+f) for f in os.listdir(mdlParams['saveDir'])]\n",
    "    else:\n",
    "        os.mkdir(mdlParams['saveDir'])\n",
    "    # Save training progress in here\n",
    "    save_dict = {}\n",
    "    save_dict['acc'] = []\n",
    "    save_dict['loss'] = []\n",
    "    save_dict['wacc'] = []\n",
    "    save_dict['auc'] = []\n",
    "    save_dict['sens'] = []\n",
    "    save_dict['spec'] = []\n",
    "    save_dict['f1'] = []\n",
    "    save_dict['step_num'] = []\n",
    "    if mdlParams['print_trainerr']:\n",
    "        save_dict_train = {}\n",
    "        save_dict_train['acc'] = []\n",
    "        save_dict_train['loss'] = []\n",
    "        save_dict_train['wacc'] = []\n",
    "        save_dict_train['auc'] = []\n",
    "        save_dict_train['sens'] = []\n",
    "        save_dict_train['spec'] = []\n",
    "        save_dict_train['f1'] = []\n",
    "        save_dict_train['step_num'] = []        \n",
    "    # Potentially calculate setMean to subtract\n",
    "    if mdlParams['subtract_set_mean'] == 1:\n",
    "        mdlParams['setMean'] = np.mean(mdlParams['images_means'][mdlParams['trainInd'],:],(0))\n",
    "        print(\"Set Mean\",mdlParams['setMean']) \n",
    "\n",
    "    # balance classes\n",
    "    if mdlParams['balance_classes'] < 3 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 11:\n",
    "        class_weights = class_weight.compute_class_weight('balanced',np.unique(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)),np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)) \n",
    "        print(\"Current class weights\",class_weights)\n",
    "        class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights)             \n",
    "    elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 4:\n",
    "        # Split training set by classes\n",
    "        not_one_hot = np.argmax(mdlParams['labels_array'],1)\n",
    "        mdlParams['class_indices'] = []\n",
    "        for i in range(mdlParams['numClasses']):\n",
    "            mdlParams['class_indices'].append(np.where(not_one_hot==i)[0])\n",
    "            # Kick out non-trainind indices\n",
    "            mdlParams['class_indices'][i] = np.setdiff1d(mdlParams['class_indices'][i],mdlParams['valInd'])\n",
    "            #print(\"Class\",i,mdlParams['class_indices'][i].shape,np.min(mdlParams['class_indices'][i]),np.max(mdlParams['class_indices'][i]),np.sum(mdlParams['labels_array'][np.int64(mdlParams['class_indices'][i]),:],0))        \n",
    "    elif mdlParams['balance_classes'] == 5 or mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 13:\n",
    "        # Other class balancing loss\n",
    "        class_weights = 1.0/np.mean(mdlParams['labels_array'][mdlParams['trainInd'],:],axis=0)\n",
    "        print(\"Current class weights\",class_weights)\n",
    "        if isinstance(mdlParams['extra_fac'], float):\n",
    "            class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "        else:\n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights) \n",
    "    elif mdlParams['balance_classes'] == 9:\n",
    "        # Only use official indicies for calculation\n",
    "        print(\"Balance 9\")\n",
    "        indices_ham = mdlParams['trainInd'][mdlParams['trainInd'] < 25331]\n",
    "        if mdlParams['numClasses'] == 9:\n",
    "            class_weights_ = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:8],axis=0)\n",
    "            #print(\"class before\",class_weights_)\n",
    "            class_weights = np.zeros([mdlParams['numClasses']])\n",
    "            class_weights[:8] = class_weights_\n",
    "            class_weights[-1] = np.max(class_weights_)\n",
    "        else:\n",
    "            class_weights = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:],axis=0)\n",
    "        print(\"Current class weights\",class_weights)             \n",
    "        if isinstance(mdlParams['extra_fac'], float):\n",
    "            class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "        else:\n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "        print(\"Current class weights with extra\",class_weights)             \n",
    "\n",
    "    # Meta scaler\n",
    "    if mdlParams.get('meta_features',None) is not None and mdlParams['scale_features']:\n",
    "        mdlParams['feature_scaler_meta'] = sklearn.preprocessing.StandardScaler().fit(mdlParams['meta_array'][mdlParams['trainInd'],:])  \n",
    "        print(\"scaler mean\",mdlParams['feature_scaler_meta'].mean_,\"var\",mdlParams['feature_scaler_meta'].var_)  \n",
    "\n",
    "    # Set up dataloaders\n",
    "    num_workers = psutil.cpu_count(logical=False)\n",
    "    # For train\n",
    "    dataset_train = utils.ISICDataset(mdlParams, 'trainInd')\n",
    "    # For val\n",
    "    dataset_val = utils.ISICDataset(mdlParams, 'valInd')\n",
    "    if mdlParams['multiCropEval'] > 0:\n",
    "        modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['multiCropEval'], shuffle=False, num_workers=num_workers, pin_memory=True)  \n",
    "    else:\n",
    "        modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['batchSize'], shuffle=False, num_workers=num_workers, pin_memory=True)               \n",
    "\n",
    "    if mdlParams['balance_classes'] == 12 or mdlParams['balance_classes'] == 13:\n",
    "        #print(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1).size(0))\n",
    "        strat_sampler = utils.StratifiedSampler(mdlParams)\n",
    "        modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], sampler=strat_sampler, num_workers=num_workers, pin_memory=True) \n",
    "    else:\n",
    "        modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True) \n",
    "    #print(\"Setdiff\",np.setdiff1d(mdlParams['trainInd'],mdlParams['trainInd']))\n",
    "    # Define model \n",
    "    modelVars['model'] = models.getModel(mdlParams)()  \n",
    "    # Load trained model\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        # Find best checkpoint\n",
    "        files = glob(mdlParams['model_load_path'] + '/CVSet' + str(cv) + '/*')\n",
    "        global_steps = np.zeros([len(files)])\n",
    "        #print(\"files\",files)\n",
    "        for i in range(len(files)):\n",
    "            # Use meta files to find the highest index\n",
    "            if 'best' not in files[i]:\n",
    "                continue\n",
    "            if 'checkpoint' not in files[i]:\n",
    "                continue                \n",
    "            # Extract global step\n",
    "            nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "            global_steps[i] = nums[-1]\n",
    "        # Create path with maximum global step found\n",
    "        chkPath = mdlParams['model_load_path'] + '/CVSet' + str(cv) + '/checkpoint_best-' + str(int(np.max(global_steps))) + '.pt'\n",
    "        print(\"Restoring lesion-trained CNN for meta data training: \",chkPath)\n",
    "        # Load\n",
    "        state = torch.load(chkPath)\n",
    "        # Initialize model\n",
    "        curr_model_dict = modelVars['model'].state_dict()\n",
    "        for name, param in state['state_dict'].items():\n",
    "            #print(name,param.shape)\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            if curr_model_dict[name].shape == param.shape:\n",
    "                curr_model_dict[name].copy_(param)\n",
    "            else:\n",
    "                print(\"not restored\",name,param.shape)\n",
    "        #modelVars['model'].load_state_dict(state['state_dict'])        \n",
    "    # Original input size\n",
    "    #if 'Dense' not in mdlParams['model_type']:\n",
    "    #    print(\"Original input size\",modelVars['model'].input_size)\n",
    "    #print(modelVars['model'])\n",
    "    if 'Dense' in mdlParams['model_type']:\n",
    "        if mdlParams['input_size'][0] != 224:\n",
    "            modelVars['model'] = utils.modify_densenet_avg_pool(modelVars['model'])\n",
    "            #print(modelVars['model'])\n",
    "        num_ftrs = modelVars['model'].classifier.in_features\n",
    "        modelVars['model'].classifier = nn.Linear(num_ftrs, mdlParams['numClasses'])\n",
    "        #print(modelVars['model'])\n",
    "    elif 'dpn' in mdlParams['model_type']:\n",
    "        num_ftrs = modelVars['model'].classifier.in_channels\n",
    "        modelVars['model'].classifier = nn.Conv2d(num_ftrs,mdlParams['numClasses'],[1,1])\n",
    "        #modelVars['model'].add_module('real_classifier',nn.Linear(num_ftrs, mdlParams['numClasses']))\n",
    "        #print(modelVars['model'])\n",
    "    elif 'efficient' in mdlParams['model_type']:\n",
    "        # Do nothing, output is prepared\n",
    "        num_ftrs = modelVars['model']._fc.in_features\n",
    "        modelVars['model']._fc = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "    elif 'wsl' in mdlParams['model_type']:\n",
    "        num_ftrs = modelVars['model'].fc.in_features\n",
    "        modelVars['model'].fc = nn.Linear(num_ftrs, mdlParams['numClasses'])          \n",
    "    else:\n",
    "        num_ftrs = modelVars['model'].last_linear.in_features\n",
    "        modelVars['model'].last_linear = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "    # Take care of meta case\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        # freeze cnn first\n",
    "        if mdlParams['freeze_cnn']:\n",
    "            # deactivate all\n",
    "            for param in modelVars['model'].parameters():\n",
    "                param.requires_grad = False            \n",
    "            if 'efficient' in mdlParams['model_type']:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model']._fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            elif 'wsl' in mdlParams['model_type']:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model'].fc.parameters():\n",
    "                    param.requires_grad = True\n",
    "            else:\n",
    "                # Activate fc\n",
    "                for param in modelVars['model'].last_linear.parameters():\n",
    "                    param.requires_grad = True                                \n",
    "        else:\n",
    "            # mark cnn parameters\n",
    "            for param in modelVars['model'].parameters():\n",
    "                param.is_cnn_param = True\n",
    "            # unmark fc\n",
    "            for param in modelVars['model']._fc.parameters():\n",
    "                param.is_cnn_param = False                              \n",
    "        # modify model\n",
    "        modelVars['model'] = models.modify_meta(mdlParams,modelVars['model'])  \n",
    "        # Mark new parameters\n",
    "        for param in modelVars['model'].parameters():\n",
    "            if not hasattr(param, 'is_cnn_param'):\n",
    "                param.is_cnn_param = False                 \n",
    "    # multi gpu support\n",
    "    if len(mdlParams['numGPUs']) > 1:\n",
    "        modelVars['model'] = nn.DataParallel(modelVars['model']) \n",
    "    modelVars['model'] = modelVars['model'].cuda()\n",
    "    #summary(modelVars['model'], modelVars['model'].input_size)# (mdlParams['input_size'][2], mdlParams['input_size'][0], mdlParams['input_size'][1]))\n",
    "\n",
    "    # Loss, with class weighting\n",
    "    if mdlParams.get('focal_loss',False):\n",
    "        modelVars['criterion'] = utils.FocalLoss(alpha=class_weights.tolist())\n",
    "    elif mdlParams['balance_classes'] == 2:\n",
    "        #modelVars['criterion'] = nn.BCEWithLogitsLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)), reduction='none')\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)),reduce=False)\n",
    "    elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 0 or mdlParams['balance_classes'] == 12:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss()\n",
    "    elif mdlParams['balance_classes'] == 8:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(reduce=False)\n",
    "    elif mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)),reduce=False)\n",
    "    elif mdlParams['balance_classes'] == 10:\n",
    "        modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'])\n",
    "    elif mdlParams['balance_classes'] == 11:\n",
    "        modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'],alpha=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "    else:\n",
    "        modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "\n",
    "    if mdlParams.get('meta_features',None) is not None:\n",
    "        if mdlParams['freeze_cnn']:\n",
    "            modelVars['optimizer'] = optim.AdamW(filter(lambda p: p.requires_grad, modelVars['model'].parameters()), lr=mdlParams['learning_rate_meta'])\n",
    "            # sanity check\n",
    "            for param in filter(lambda p: p.requires_grad, modelVars['model'].parameters()):\n",
    "                print(param.name,param.shape)\n",
    "        else:\n",
    "            modelVars['optimizer'] = optim.AdamW([\n",
    "                                                {'params': filter(lambda p: not p.is_cnn_param, modelVars['model'].parameters()), 'lr': mdlParams['learning_rate_meta']},\n",
    "                                                {'params': filter(lambda p: p.is_cnn_param, modelVars['model'].parameters()), 'lr': mdlParams['learning_rate']}\n",
    "                                                ], lr=mdlParams['learning_rate'])\n",
    "    else:\n",
    "        modelVars['optimizer'] = optim.AdamW(modelVars['model'].parameters(), lr=mdlParams['learning_rate'])\n",
    "\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    #modelVars['scheduler'] = lr_scheduler.StepLR(modelVars['optimizer'], step_size=mdlParams['lowerLRAfter'], gamma=1/np.float32(mdlParams['LRstep']))\n",
    "    modelVars['scheduler'] = lr_scheduler.OneCycleLR(modelVars['optimizer'], max_lr=mdlParams['learning_rate'],\n",
    "                                                     epochs=mdlParams['training_steps'],\n",
    "                                                     steps_per_epoch=len(dataset_train)//mdlParams['batchSize'])\n",
    "    # Define softmax\n",
    "    modelVars['softmax'] = nn.Softmax(dim=1)\n",
    "\n",
    "    # Set up training\n",
    "    # loading from checkpoint\n",
    "    if load_old:\n",
    "        # Find last, not last best checkpoint\n",
    "        files = glob(mdlParams['saveDir']+'/*')\n",
    "        global_steps = np.zeros([len(files)])\n",
    "        for i in range(len(files)):\n",
    "            # Use meta files to find the highest index\n",
    "            if 'best' in files[i]:\n",
    "                continue\n",
    "            if 'checkpoint-' not in files[i]:\n",
    "                continue                \n",
    "            # Extract global step\n",
    "            nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "            global_steps[i] = nums[-1]\n",
    "        # Create path with maximum global step found\n",
    "        chkPath = mdlParams['saveDir'] + '/checkpoint-' + str(int(np.max(global_steps))) + '.pt'\n",
    "        print(\"Restoring: \",chkPath)\n",
    "        # Load\n",
    "        state = torch.load(chkPath)\n",
    "        # Initialize model and optimizer\n",
    "        modelVars['model'].load_state_dict(state['state_dict'])\n",
    "        modelVars['optimizer'].load_state_dict(state['optimizer'])     \n",
    "        start_epoch = state['epoch']+1\n",
    "        mdlParams['valBest'] = state.get('valBest',1000)\n",
    "        mdlParams['lastBestInd'] = state.get('lastBestInd',int(np.max(global_steps)))\n",
    "    else:\n",
    "        start_epoch = 1\n",
    "        mdlParams['lastBestInd'] = -1\n",
    "        # Track metrics for saving best model\n",
    "        mdlParams['valBest'] = 1000\n",
    "\n",
    "    # Num batches\n",
    "    numBatchesTrain = int(math.floor(len(mdlParams['trainInd'])/mdlParams['batchSize']))\n",
    "    print(\"Train batches\",numBatchesTrain)\n",
    "\n",
    "    mdlParams['classification'] = True\n",
    "    mdlParams['print_trainerr'] = False\n",
    "\n",
    "    # Run training\n",
    "    start_time = time.time()\n",
    "    print(\"Start training...\")\n",
    "    for step in tqdm(range(start_epoch, mdlParams['training_steps']+1)):\n",
    "        # One Epoch of training\n",
    "#         if step >= mdlParams['lowerLRat']-mdlParams['lowerLRAfter']:\n",
    "#             modelVars['scheduler'].step()\n",
    "        modelVars['model'].train()      \n",
    "        for j, (inputs, labels, indices) in tqdm(enumerate(modelVars['dataloader_trainInd']), total=numBatchesTrain):    \n",
    "            #print(indices)                  \n",
    "            #t_load = time.time() \n",
    "            # Run optimization        \n",
    "            if mdlParams.get('meta_features',None) is not None: \n",
    "                inputs[0] = inputs[0].cuda()\n",
    "                inputs[1] = inputs[1].cuda()\n",
    "            else:\n",
    "                inputs = inputs.cuda()\n",
    "            #print(inputs.shape)\n",
    "            labels = labels.cuda()        \n",
    "            # zero the parameter gradients\n",
    "            modelVars['optimizer'].zero_grad()             \n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(True):             \n",
    "                if mdlParams.get('aux_classifier',False):\n",
    "                    outputs, outputs_aux = modelVars['model'](inputs) \n",
    "                    loss1 = modelVars['criterion'](outputs, labels)\n",
    "                    labels_aux = labels.repeat(mdlParams['multiCropTrain'])\n",
    "                    loss2 = modelVars['criterion'](outputs_aux, labels_aux) \n",
    "                    loss = loss1 + mdlParams['aux_classifier_loss_fac']*loss2     \n",
    "                else:               \n",
    "                    #print(\"load\",time.time()-t_load)    \n",
    "                    #t_fwd = time.time()   \n",
    "                    outputs = modelVars['model'](inputs)     \n",
    "                    #print(\"forward\",time.time()-t_fwd)     \n",
    "                    #t_bwd = time.time()\n",
    "                    #outputs = outputs.sum(dim=1, keepdim=True)\n",
    "                    #outputs = outputs.squeeze()\n",
    "                    #labels = labels.float()\n",
    "                    #labels = labels.float()\n",
    "                    #print(outputs.shape, outputs)\n",
    "                    #print(labels.shape, labels)\n",
    "                    loss = modelVars['criterion'](outputs, labels).sum()\n",
    "                    #print(loss)\n",
    "                # Perhaps adjust weighting of the loss by the specific index\n",
    "                if mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 8:\n",
    "                    #loss = loss.cpu()\n",
    "                    indices = indices.numpy()\n",
    "                    loss = loss*torch.cuda.FloatTensor(mdlParams['loss_fac_per_example'][indices].astype(np.float32))\n",
    "                    loss = torch.mean(loss)\n",
    "                    #loss = loss.cuda()\n",
    "                # backward + optimize only if in training phase\n",
    "                loss.backward()                 \n",
    "                modelVars['optimizer'].step()\n",
    "                modelVars['scheduler'].step()                \n",
    "                #print(\"backward\",time.time()-t_bwd)                             \n",
    "        if step % mdlParams['display_step'] == 0 or step == 1:\n",
    "            # Calculate evaluation metrics\n",
    "            if mdlParams['classification']:\n",
    "                # Adjust model state\n",
    "                modelVars['model'].eval()\n",
    "                # Get metrics\n",
    "                loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, _ = utils.getErrClassification_mgpu(mdlParams, eval_set, modelVars)\n",
    "                # Save in mat\n",
    "                save_dict['loss'].append(loss)\n",
    "                save_dict['acc'].append(accuracy)\n",
    "                save_dict['wacc'].append(waccuracy)\n",
    "                save_dict['auc'].append(auc)\n",
    "                save_dict['sens'].append(sensitivity)\n",
    "                save_dict['spec'].append(specificity)\n",
    "                save_dict['f1'].append(f1)\n",
    "                save_dict['step_num'].append(step)\n",
    "                if os.path.isfile(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat'):\n",
    "                    os.remove(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat')                \n",
    "                io.savemat(mdlParams['saveDir'] + '/progression_'+eval_set+'.mat',save_dict)                \n",
    "            eval_metric = -np.mean(waccuracy)\n",
    "            # Check if we have a new best value\n",
    "            if eval_metric < mdlParams['valBest']:\n",
    "                mdlParams['valBest'] = eval_metric\n",
    "                if mdlParams['classification']:\n",
    "                    allData['f1Best'][cv] = f1\n",
    "                    allData['sensBest'][cv] = sensitivity\n",
    "                    allData['specBest'][cv] = specificity\n",
    "                    allData['accBest'][cv] = accuracy\n",
    "                    allData['waccBest'][cv] = waccuracy\n",
    "                    allData['aucBest'][cv] = auc\n",
    "                oldBestInd = mdlParams['lastBestInd']\n",
    "                mdlParams['lastBestInd'] = step\n",
    "                allData['convergeTime'][cv] = step\n",
    "                # Save best predictions\n",
    "                allData['bestPred'][cv] = predictions\n",
    "                allData['targets'][cv] = targets\n",
    "                # Write to File\n",
    "                with open(mdlParams['saveDirBase'] + '/CV.pkl', 'wb') as f:\n",
    "                    pickle.dump(allData, f, pickle.HIGHEST_PROTOCOL)                 \n",
    "                # Delte previously best model\n",
    "                if os.path.isfile(mdlParams['saveDir'] + '/checkpoint_best-' + str(oldBestInd) + '.pt'):\n",
    "                    os.remove(mdlParams['saveDir'] + '/checkpoint_best-' + str(oldBestInd) + '.pt')\n",
    "                # Save currently best model\n",
    "                state = {'epoch': step, 'valBest': mdlParams['valBest'], 'lastBestInd': mdlParams['lastBestInd'], 'state_dict': modelVars['model'].state_dict(),'optimizer': modelVars['optimizer'].state_dict()}\n",
    "                torch.save(state, mdlParams['saveDir'] + '/checkpoint_best-' + str(step) + '.pt')               \n",
    "\n",
    "            # If its not better, just save it delete the last checkpoint if it is not current best one\n",
    "            # Save current model\n",
    "            state = {'epoch': step, 'valBest': mdlParams['valBest'], 'lastBestInd': mdlParams['lastBestInd'], 'state_dict': modelVars['model'].state_dict(),'optimizer': modelVars['optimizer'].state_dict()}\n",
    "            torch.save(state, mdlParams['saveDir'] + '/checkpoint-' + str(step) + '.pt')                           \n",
    "            # Delete last one\n",
    "            if step == mdlParams['display_step']:\n",
    "                lastInd = 1\n",
    "            else:\n",
    "                lastInd = step-mdlParams['display_step']\n",
    "            if os.path.isfile(mdlParams['saveDir'] + '/checkpoint-' + str(lastInd) + '.pt'):\n",
    "                os.remove(mdlParams['saveDir'] + '/checkpoint-' + str(lastInd) + '.pt')       \n",
    "            # Duration so far\n",
    "            duration = time.time() - start_time                          \n",
    "            # Print\n",
    "            if mdlParams['classification']:\n",
    "                print(\"\\n\")\n",
    "                print(\"Config:\",sys.argv[2])\n",
    "                print('Fold: %d Epoch: %d/%d (%d h %d m %d s)' % (cv,step,mdlParams['training_steps'], int(duration/3600), int(np.mod(duration,3600)/60), int(np.mod(np.mod(duration,3600),60))) + time.strftime(\"%d.%m.-%H:%M:%S\", time.localtime()))\n",
    "                print(\"Loss on \",eval_set,\"set: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1,\" (best WACC: \",-mdlParams['valBest'],\" at Epoch \",mdlParams['lastBestInd'],\")\")\n",
    "                print(\"Auc\",auc,\"Mean AUC\",np.mean(auc))\n",
    "                print(\"Per Class Acc\",waccuracy,\"Weighted Accuracy\",np.mean(waccuracy))\n",
    "                print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "                print(\"Confusion Matrix\")\n",
    "                print(conf_matrix)\n",
    "                # Potentially peek at test error\n",
    "                if mdlParams['peak_at_testerr']:              \n",
    "                    loss, accuracy, sensitivity, specificity, _, f1, _, _, _, _, _ = utils.getErrClassification_mgpu(mdlParams, 'testInd', modelVars)\n",
    "                    print(\"Test loss: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1)\n",
    "                    print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "                # Potentially print train err\n",
    "                if mdlParams['print_trainerr'] and 'train' not in eval_set:                \n",
    "                    loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, _ = utils.getErrClassification_mgpu(mdlParams, 'trainInd', modelVars)\n",
    "                    # Save in mat\n",
    "                    save_dict_train['loss'].append(loss)\n",
    "                    save_dict_train['acc'].append(accuracy)\n",
    "                    save_dict_train['wacc'].append(waccuracy)\n",
    "                    save_dict_train['auc'].append(auc)\n",
    "                    save_dict_train['sens'].append(sensitivity)\n",
    "                    save_dict_train['spec'].append(specificity)\n",
    "                    save_dict_train['f1'].append(f1)\n",
    "                    save_dict_train['step_num'].append(step)\n",
    "                    if os.path.isfile(mdlParams['saveDir'] + '/progression_trainInd.mat'):\n",
    "                        os.remove(mdlParams['saveDir'] + '/progression_trainInd.mat')                \n",
    "                    scipy.io.savemat(mdlParams['saveDir'] + '/progression_trainInd.mat',save_dict_train)                     \n",
    "                    print(\"Train loss: \",loss,\" Accuracy: \",accuracy,\" F1: \",f1)\n",
    "                    print(\"Sensitivity: \",sensitivity,\"Specificity\",specificity)\n",
    "    # Free everything in modelvars\n",
    "    modelVars.clear()\n",
    "    # After CV Training: print CV results and save them\n",
    "    print(\"Best F1:\",allData['f1Best'][cv])\n",
    "    print(\"Best Sens:\",allData['sensBest'][cv])\n",
    "    print(\"Best Spec:\",allData['specBest'][cv])\n",
    "    print(\"Best Acc:\",allData['accBest'][cv])\n",
    "    print(\"Best Per Class Accuracy:\",allData['waccBest'][cv])\n",
    "    print(\"Best Weighted Acc:\",np.mean(allData['waccBest'][cv]))\n",
    "    print(\"Best AUC:\",allData['aucBest'][cv])\n",
    "    print(\"Best Mean AUC:\",np.mean(allData['aucBest'][cv]))    \n",
    "    print(\"Convergence Steps:\",allData['convergeTime'][cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45824,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlParams['trainInd'][.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2158, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdlParams['labels_array'].shape # [mdlParams['trainInd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2158"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mdlParams['labels_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

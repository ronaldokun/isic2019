{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANDA EfficientNet-B0 Baseline with 36 x tiles_256\n",
    "\n",
    "Hi everyone,\n",
    "\n",
    "I'm here to show you how to train a single efficientnet-b0 model to get LB 0.87\n",
    "\n",
    "Inference kernel is https://www.kaggle.com/haqishen/panda-inference-w-36-tiles-256\n",
    "\n",
    "If you find find any of the following idea helps, please upvote me, THANKS!\n",
    "\n",
    "# Summary of This Baseline\n",
    "\n",
    "* Using tiling method based on https://www.kaggle.com/iafoss/panda-16x128x128-tiles\n",
    "    * Simply setting the `N = 36` and `sz=256` then extract from median resolution\n",
    "* Create 6x6 big image from 36 tiles\n",
    "* Efficientnet-B0\n",
    "* Binning label\n",
    "    * E.g.\n",
    "        * `label = [0,0,0,0,0]` means `isup_grade = 0`\n",
    "        * `label = [1,1,1,0,0]` means `isup_grade = 3`\n",
    "        * `label = [1,1,1,1,1]` means `isup_grade = 5`\n",
    "* BCE loss\n",
    "* Augmentation on both tile level and big image level\n",
    "* CosineAnnealingLR for one round\n",
    "\n",
    "# MEMO\n",
    "\n",
    "The full training process need over `10h` to run so you should run it on your own machine.\n",
    "\n",
    "# Update\n",
    "* Version 1\n",
    "    * Baseline\n",
    "* Version 2, 3\n",
    "    * Add some Markdown Text\n",
    "* Version 4\n",
    "    * Fix `init_lr` from 3e-5 to 3e-4\n",
    "* Version 5\n",
    "    * Add warmup scheduler\n",
    "    * Add training log for this version\n",
    "* Version 6\n",
    "    * Fix the bug that train from scratch. Now it's train from ImageNet pretrained weights. Actually I haven't tried train from scratch yet.\n",
    "* Version 7, 8\n",
    "    * Update accuracy calculate.\n",
    "    * Fix tiny bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# sys.path = [\n",
    "#     '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master',\n",
    "# ] + sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from torchvision import transforms\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from radam import *\n",
    "from csvlogger import *\n",
    "from mish_activation import *\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import albumentations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image(fn, mode=None, **kwargs):\n",
    "#     \"Open and load a `PIL.Image` and convert to `mode`\"\n",
    "#     im = PIL.Image.open(fn, **kwargs)\n",
    "#     im.load()\n",
    "#     im = im._new(im.im)\n",
    "#     return im.convert(mode) if mode else im\n",
    "\n",
    "# # Cell\n",
    "# def image2tensor(img):\n",
    "#     \"Transform image to byte tensor in `c*h*w` dim order.\"\n",
    "#     res = tensor(img)\n",
    "#     if res.dim()==2: res = res.unsqueeze(-1)\n",
    "#     return res.permute(2,0,1)\n",
    "\n",
    "# def pil2numpy(image,dtype:np.dtype):\n",
    "#     \"Convert PIL style `image` array to torch style image tensor.\"\n",
    "#     a = np.asarray(image)\n",
    "# #     if a.ndim==2 : a = np.expand_dims(a,2)\n",
    "# #     a = np.transpose(a, (1, 0, 2))\n",
    "# #     a = np.transpose(a, (2, 1, 0))\n",
    "#     return a.astype(dtype, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pil2tensor(image,dtype:np.dtype):\n",
    "#     \"Convert PIL style `image` array to torch style image tensor.\"\n",
    "#     a = np.asarray(image)\n",
    "#     if a.ndim==2 : a = np.expand_dims(a,2)\n",
    "#     a = np.transpose(a, (1, 0, 2))\n",
    "#     a = np.transpose(a, (2, 1, 0))\n",
    "#     return torch.from_numpy(a.astype(dtype, copy=False) )\n",
    "\n",
    "# def open_image(fn, div=False, convert_mode='RGB'):\n",
    "#     x = load_image(fn, mode=convert_mode)\n",
    "#     x = pil2numpy(x,np.float32)\n",
    "#     if div: \n",
    "#         #x.div_(255)\n",
    "#         x = x/255.0\n",
    "#     #return 1.0 - x #invert image for zero padding\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('/content/clouderizer/panda/data')\n",
    "TRAIN = '/content/clouderizer/panda/data/train/'\n",
    "df_train = pd.read_csv(os.path.join(data_dir, 'train.csv')).sort_values('image_id')\n",
    "tiles_folder = data_dir / 'train'\n",
    "\n",
    "enet_type = 'efficientnet-b0'\n",
    "kernel_type = enet_type\n",
    "fold = 0\n",
    "tile_size = 128\n",
    "image_size = 128\n",
    "n_tiles = 16\n",
    "N = 16\n",
    "batch_size = 2\n",
    "num_workers = 4\n",
    "out_dim = 6\n",
    "init_lr = 3e-4\n",
    "warmup_factor = 10\n",
    "\n",
    "warmup_epo = 1\n",
    "n_epochs = 30\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "      <th>isup_grade</th>\n",
       "      <th>gleason_score</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005f7aaab2800f6170c399693a96917</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000920ad0b612851f8e01bcc880d9b3d</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0018ae58b01bdadc8e347995b69f99aa</td>\n",
       "      <td>radboud</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001c62abd11fa4b57bf7a6c603a11bb9</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001d865e65ef5d2579c190a0e0350d8f</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_id data_provider  ...  gleason_score fold\n",
       "0  0005f7aaab2800f6170c399693a96917    karolinska  ...            0+0    4\n",
       "1  000920ad0b612851f8e01bcc880d9b3d    karolinska  ...            0+0    0\n",
       "2  0018ae58b01bdadc8e347995b69f99aa       radboud  ...            4+4    3\n",
       "3  001c62abd11fa4b57bf7a6c603a11bb9    karolinska  ...            4+4    4\n",
       "4  001d865e65ef5d2579c190a0e0350d8f    karolinska  ...            0+0    4\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "df_train['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train['isup_grade'])):\n",
    "    df_train.loc[valid_idx, 'fold'] = i\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, backbone, out_dim):\n",
    "        super().__init__()\n",
    "        self.enet = EfficientNet.from_pretrained(backbone)\n",
    "        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n",
    "        self.enet._fc = nn.Identity()\n",
    "\n",
    "    def extract(self, x):\n",
    "        return self.enet(x)\n",
    "\n",
    "    def forward(self, *x):\n",
    "        shape = x[0].shape\n",
    "        n = len(x)\n",
    "        x = torch.stack(x,1).view(-1,shape[1],shape[2],shape[3])\n",
    "        #x: bs*N x 3 x 128 x 128\n",
    "        x = self.enet(x)\n",
    "        #x: bs*N x C x 4 x 4\n",
    "        shape = x.shape\n",
    "        #concatenate the output for tiles into a single map\n",
    "        x = x.view(-1,n,shape[1],shape[2],shape[3]).permute(0,2,1,3,4).contiguous()\\\n",
    "          .view(-1,shape[1],shape[2]*n,shape[3])\n",
    "        #x: bs x C x N*4 x 4\n",
    "        x = self.myfc(x)\n",
    "        #x: bs x n\n",
    "        return x\n",
    "#         x = self.extract(x)\n",
    "#         x = self.myfc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tiles = [[f'{TRAIN}{f}_{i}.png' for i in range(n_tiles)] for f in df_train.image_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_files = set(str(p) for p in Path(TRAIN).iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tiles(img, mode=0):\n",
    "#         result = []\n",
    "#         h, w, c = img.shape\n",
    "#         pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "#         pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "\n",
    "#         img2 = np.pad(img,[[pad_h // 2, pad_h - pad_h // 2], [pad_w // 2,pad_w - pad_w//2], [0,0]], constant_values=255)\n",
    "#         img3 = img2.reshape(\n",
    "#             img2.shape[0] // tile_size,\n",
    "#             tile_size,\n",
    "#             img2.shape[1] // tile_size,\n",
    "#             tile_size,\n",
    "#             3\n",
    "#         )\n",
    "\n",
    "#         img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n",
    "#         n_tiles_with_info = (img3.reshape(img3.shape[0],-1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n",
    "#         if len(img3) < n_tiles:\n",
    "#             img3 = np.pad(img3,[[0,n_tiles-len(img3)],[0,0],[0,0],[0,0]], constant_values=255)\n",
    "#         idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]\n",
    "#         img3 = img3[idxs]\n",
    "#         for i in range(len(img3)):\n",
    "#             result.append({'img':img3[i], 'idx':i})\n",
    "#         return result, n_tiles_with_info >= n_tiles\n",
    "\n",
    "\n",
    "# class PANDADataset(Dataset):\n",
    "#     def __init__(self,\n",
    "#                  df,\n",
    "#                  image_size,\n",
    "#                  n_tiles=n_tiles,\n",
    "#                  tile_mode=0,\n",
    "#                  rand=False,\n",
    "#                  transform=None,\n",
    "#                 ):\n",
    "\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.image_size = image_size\n",
    "#         self.n_tiles = n_tiles\n",
    "#         self.tile_mode = tile_mode\n",
    "#         self.rand = rand\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.df.shape[0]\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         row = self.df.iloc[index]\n",
    "#         img_id = row.image_id\n",
    "        \n",
    "#         #tiff_file = os.path.join(image_folder, f'{img_id}.tiff')\n",
    "#         #image = skimage.io.MultiImage(tiff_file)[1]\n",
    "#         #tiles, OK = get_tiles(image, self.tile_mode)\n",
    "        \n",
    "#         tiles = all_tiles[index]\n",
    "\n",
    "#         if self.rand:\n",
    "#             idxes = np.random.choice(list(range(self.n_tiles)), self.n_tiles, replace=False)\n",
    "#         else:\n",
    "#             idxes = list(range(self.n_tiles))\n",
    "\n",
    "#         n_row_tiles = int(np.sqrt(self.n_tiles))\n",
    "#         images = np.zeros((image_size * n_row_tiles, image_size * n_row_tiles, 3))\n",
    "#         for h in range(n_row_tiles):\n",
    "#             for w in range(n_row_tiles):\n",
    "#                 i = h * n_row_tiles + w\n",
    "                \n",
    "#                 if len(tiles) > idxes[i] and tiles[idxes[i]] in all_files:\n",
    "#                     this_img = open_image(tiles[idxes[i]])\n",
    "#                 else:\n",
    "#                      this_img = np.ones((self.image_size, self.image_size, 3)).astype(np.uint8) * 255\n",
    "#                 this_img = 255 - this_img\n",
    "#                 if self.transform is not None:\n",
    "#                     this_img = self.transform(image=this_img)['image'] #['image']\n",
    "#                 h1 = h * image_size\n",
    "#                 w1 = w * image_size\n",
    "#                 images[h1:h1+image_size, w1:w1+image_size] = this_img\n",
    "\n",
    "#         if self.transform is not None:\n",
    "#             images = self.transform(image=images)['image']\n",
    "#         images = images.astype(np.float32)\n",
    "#         images /= 255\n",
    "#         images = images.transpose(2, 0, 1)\n",
    "\n",
    "#         label = np.zeros(5).astype(np.float32)\n",
    "#         label[:row.isup_grade] = 1.\n",
    "#         return torch.tensor(images), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms_train = albumentations.Compose([\n",
    "#     albumentations.Transpose(p=0.5),\n",
    "#     albumentations.VerticalFlip(p=0.5),\n",
    "#     albumentations.HorizontalFlip(p=0.5),\n",
    "# ])\n",
    "# transforms_val = albumentations.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_show = PANDADataset(df_train, image_size, n_tiles, 0, transform=transforms_train)\n",
    "# from pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 20,10\n",
    "# for i in range(2):\n",
    "#     f, axarr = plt.subplots(1,5)\n",
    "#     for p in range(5):\n",
    "#         idx = np.random.randint(0, len(dataset_show))\n",
    "#         img, label = dataset_show[idx]\n",
    "#         axarr[p].imshow(1. -  img.transpose(0, 1).transpose(1,2).squeeze())\n",
    "#         axarr[p].set_title(str(sum(label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([1.0-0.90949707, 1.0-0.8188697, 1.0-0.87795304])\n",
    "std = torch.tensor([0.36357649, 0.49984502, 0.40477625])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below (in the hidden cell) creates ImageItemList capable of loading multiple tiles of an image. It is specific for fast.ai, and pure Pytorch code would be much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def open_image(fn:PathOrStr, div:bool=True, convert_mode:str='RGB', cls:type=Image,\n",
    "        after_open:Callable=None)->Image:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning) # EXIF warning from TiffPlugin\n",
    "        x = PIL.Image.open(fn).convert(convert_mode)\n",
    "    if after_open: x = after_open(x)\n",
    "    x = pil2tensor(x,np.float32)\n",
    "    if div: x.div_(255)\n",
    "    return cls(1.0-x) #invert image for zero padding\n",
    "\n",
    "class MImage(ItemBase):\n",
    "    def __init__(self, imgs):\n",
    "        self.obj, self.data = \\\n",
    "          (imgs), [(imgs[i].data - mean[...,None,None])/std[...,None,None] for i in range(len(imgs))]\n",
    "    \n",
    "    def apply_tfms(self, tfms,*args, **kwargs):\n",
    "        for i in range(len(self.obj)):\n",
    "            self.obj[i] = self.obj[i].apply_tfms(tfms, *args, **kwargs)\n",
    "            self.data[i] = (self.obj[i].data - mean[...,None,None])/std[...,None,None]\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self): return f'{self.__class__.__name__} {img.shape for img in self.obj}'\n",
    "    def to_one(self):\n",
    "        img = torch.stack(self.data,1)\n",
    "        img = img.view(3,-1,N,sz,sz).permute(0,1,3,2,4).contiguous().view(3,-1,sz*N)\n",
    "        return Image(1.0 - (mean[...,None,None]+img*std[...,None,None]))\n",
    "\n",
    "class MImageItemList(ImageList):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def __len__(self)->int: return len(self.items) or 1 \n",
    "    \n",
    "    def get(self, i):\n",
    "        fn = Path(self.items[i])\n",
    "        fnames = [Path(str(fn)+'_'+str(i)+'.png')for i in range(N)]\n",
    "        imgs = [open_image(fname, convert_mode=self.convert_mode, after_open=self.after_open)\n",
    "               for fname in fnames]\n",
    "        return MImage(imgs)\n",
    "\n",
    "    def reconstruct(self, t):\n",
    "        return MImage([mean[...,None,None]+_t*std[...,None,None] for _t in t])\n",
    "    \n",
    "    def show_xys(self, xs, ys, figsize:Tuple[int,int]=(300,50), **kwargs):\n",
    "        rows = min(len(xs),8)\n",
    "        fig, axs = plt.subplots(rows,1,figsize=figsize)\n",
    "        for i, ax in enumerate(axs.flatten() if rows > 1 else [axs]):\n",
    "            xs[i].to_one().show(ax=ax, y=ys[i], **kwargs)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "\n",
    "#collate function to combine multiple images into one tensor\n",
    "def MImage_collate(batch:ItemsList)->Tensor:\n",
    "    result = torch.utils.data.dataloader.default_collate(to_data(batch))\n",
    "    if isinstance(result[0],list):\n",
    "        result = [torch.stack(result[0],1),result[1]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fold=0):\n",
    "    return (MImageItemList.from_df(df_train, path='/', folder=TRAIN, cols='image_id')\n",
    "      .split_by_idx(df_train.index[df_train.fold == fold].tolist())\n",
    "      .label_from_df(cols=['isup_grade'])\n",
    "      .transform(get_transforms(flip_vert=True,max_rotate=15),size=image_size,padding_mode='zeros')\n",
    "      .databunch(bs=batch_size,num_workers=num_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        \n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        loss_func = criterion\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(*data)\n",
    "        loss = loss_func(logits, *target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(loader, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    PREDS = []\n",
    "    TARGETS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            logits = model(*data)\n",
    "\n",
    "            loss = criterion(logits, *target)\n",
    "\n",
    "            pred = logits.sigmoid().sum(1).detach().round()\n",
    "            LOGITS.append(logits)\n",
    "            PREDS.append(pred)\n",
    "            TARGETS.append(*target.sum(1))\n",
    "\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "    LOGITS = torch.cat(LOGITS).cpu().numpy()\n",
    "    PREDS = torch.cat(PREDS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "    acc = (PREDS == TARGETS).mean() * 100.\n",
    "    \n",
    "    qwk = cohen_kappa_score(PREDS, TARGETS, weights='quadratic')\n",
    "    qwk_k = cohen_kappa_score(PREDS[df_valid['data_provider'] == 'karolinska'], df_valid[df_valid['data_provider'] == 'karolinska'].isup_grade.values, weights='quadratic')\n",
    "    qwk_r = cohen_kappa_score(PREDS[df_valid['data_provider'] == 'radboud'], df_valid[df_valid['data_provider'] == 'radboud'].isup_grade.values, weights='quadratic')\n",
    "    print('qwk', qwk, 'qwk_k', qwk_k, 'qwk_r', qwk_r)\n",
    "\n",
    "    if get_output:\n",
    "        return LOGITS\n",
    "    else:\n",
    "        return val_loss, acc, qwk\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader & Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_idx = np.where((df_train['fold'] != fold))[0]\n",
    "# valid_idx = np.where((df_train['fold'] == fold))[0]\n",
    "\n",
    "# df_this  = df_train.loc[train_idx]\n",
    "# df_valid = df_train.loc[valid_idx]\n",
    "\n",
    "# dataset_train = PANDADataset(df_this , image_size, n_tiles, transform=transforms_train)\n",
    "# dataset_valid = PANDADataset(df_valid, image_size, n_tiles, transform=transforms_val)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=RandomSampler(dataset_train), num_workers=num_workers)\n",
    "# valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, sampler=SequentialSampler(dataset_valid), num_workers=num_workers)\n",
    "\n",
    "data = get_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.train_dl\n",
    "valid_load = data.valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(enet_type, out_dim=out_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr/warmup_factor)\n",
    "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-warmup_epo)\n",
    "scheduler = GradualWarmupScheduler(optimizer, multiplier=warmup_factor, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qwk_max = 0.\n",
    "best_file = f'{kernel_type}_best_fold{fold}.pth'\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    print(time.ctime(), 'Epoch:', epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = train_epoch(train_loader, optimizer)\n",
    "    val_loss, acc, qwk = val_epoch(valid_loader)\n",
    "\n",
    "    content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, val loss: {np.mean(val_loss):.5f}, acc: {(acc):.5f}, qwk: {(qwk):.5f}'\n",
    "    print(content)\n",
    "    with open(f'log_{kernel_type}.txt', 'a') as appender:\n",
    "        appender.write(content + '\\n')\n",
    "\n",
    "    if qwk > qwk_max:\n",
    "        print('score2 ({:.6f} --> {:.6f}).  Saving model ...'.format(qwk_max, qwk))\n",
    "        torch.save(model.state_dict(), best_file)\n",
    "        qwk_max = qwk\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(f'{kernel_type}_final_fold{fold}.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Local Train Log\n",
    "\n",
    "\n",
    "```\n",
    "Tue June 2 15:39:21 2020 Epoch 1, lr: 0.0000300, train loss: 0.42295, val loss: 0.29257, acc: 47.50471, qwk: 0.77941\n",
    "Tue June 2 15:51:56 2020 Epoch 2, lr: 0.0003000, train loss: 0.34800, val loss: 0.48723, acc: 29.09605, qwk: 0.58493\n",
    "Tue June 2 16:04:28 2020 Epoch 3, lr: 0.0003000, train loss: 0.29207, val loss: 0.27091, acc: 52.49529, qwk: 0.81714\n",
    "Tue June 2 16:17:01 2020 Epoch 4, lr: 0.0002965, train loss: 0.26521, val loss: 0.26736, acc: 57.15631, qwk: 0.80364\n",
    "Tue June 2 16:29:33 2020 Epoch 5, lr: 0.0002921, train loss: 0.24412, val loss: 0.24422, acc: 56.07345, qwk: 0.84068\n",
    "Tue June 2 16:42:05 2020 Epoch 6, lr: 0.0002861, train loss: 0.23085, val loss: 0.25306, acc: 58.05085, qwk: 0.84429\n",
    "Tue June 2 16:54:38 2020 Epoch 7, lr: 0.0002785, train loss: 0.21998, val loss: 0.21920, acc: 62.14689, qwk: 0.86278\n",
    "Tue June 2 17:07:10 2020 Epoch 8, lr: 0.0002694, train loss: 0.21062, val loss: 0.23400, acc: 61.91149, qwk: 0.86170\n",
    "Tue June 2 17:19:47 2020 Epoch 9, lr: 0.0002589, train loss: 0.20040, val loss: 0.27417, acc: 57.10923, qwk: 0.81771\n",
    "Tue June 2 17:32:25 2020 Epoch 10, lr: 0.0002471, train loss: 0.18900, val loss: 0.26732, acc: 64.92467, qwk: 0.84131\n",
    "Tue June 2 17:45:05 2020 Epoch 11, lr: 0.0002342, train loss: 0.18640, val loss: 0.21936, acc: 63.27684, qwk: 0.86580\n",
    "Tue June 2 17:57:42 2020 Epoch 12, lr: 0.0002203, train loss: 0.17387, val loss: 0.22863, acc: 61.25235, qwk: 0.86871\n",
    "Tue June 2 18:10:23 2020 Epoch 13, lr: 0.0002055, train loss: 0.16491, val loss: 0.23071, acc: 66.85499, qwk: 0.87892\n",
    "Tue June 2 18:23:00 2020 Epoch 14, lr: 0.0001901, train loss: 0.15448, val loss: 0.24338, acc: 68.45574, qwk: 0.87342\n",
    "Tue June 2 18:35:39 2020 Epoch 15, lr: 0.0001743, train loss: 0.14536, val loss: 0.22043, acc: 65.11299, qwk: 0.87169\n",
    "Tue June 2 18:48:18 2020 Epoch 16, lr: 0.0001581, train loss: 0.13918, val loss: 0.22007, acc: 67.65537, qwk: 0.88284\n",
    "Tue June 2 19:00:55 2020 Epoch 17, lr: 0.0001419, train loss: 0.13121, val loss: 0.24287, acc: 66.71375, qwk: 0.86357\n",
    "Tue June 2 19:13:35 2020 Epoch 18, lr: 0.0001257, train loss: 0.12249, val loss: 0.21583, acc: 66.80791, qwk: 0.88478\n",
    "Tue June 2 19:26:14 2020 Epoch 19, lr: 0.0001099, train loss: 0.11325, val loss: 0.21401, acc: 71.13936, qwk: 0.89178\n",
    "Tue June 2 19:38:55 2020 Epoch 20, lr: 0.0000945, train loss: 0.10602, val loss: 0.21250, acc: 70.00942, qwk: 0.89256\n",
    "Tue June 2 19:51:32 2020 Epoch 21, lr: 0.0000797, train loss: 0.09965, val loss: 0.21149, acc: 70.33898, qwk: 0.89590\n",
    "Tue June 2 20:03:59 2020 Epoch 22, lr: 0.0000658, train loss: 0.09425, val loss: 0.22203, acc: 70.76271, qwk: 0.89493\n",
    "Tue June 2 20:16:28 2020 Epoch 23, lr: 0.0000529, train loss: 0.08843, val loss: 0.22948, acc: 71.70433, qwk: 0.89304\n",
    "Tue June 2 20:28:56 2020 Epoch 24, lr: 0.0000411, train loss: 0.08448, val loss: 0.21200, acc: 71.18644, qwk: 0.89947\n",
    "Tue June 2 20:41:25 2020 Epoch 25, lr: 0.0000306, train loss: 0.07898, val loss: 0.21873, acc: 72.55179, qwk: 0.90021\n",
    "Tue June 2 20:53:53 2020 Epoch 26, lr: 0.0000215, train loss: 0.07369, val loss: 0.21842, acc: 72.64595, qwk: 0.90240\n",
    "Tue June 2 21:06:20 2020 Epoch 27, lr: 0.0000139, train loss: 0.07264, val loss: 0.21501, acc: 73.21092, qwk: 0.90450\n",
    "Tue June 2 21:18:49 2020 Epoch 28, lr: 0.0000079, train loss: 0.06950, val loss: 0.21616, acc: 73.35217, qwk: 0.90264\n",
    "Tue June 2 21:31:16 2020 Epoch 29, lr: 0.0000035, train loss: 0.06787, val loss: 0.21195, acc: 73.11676, qwk: 0.90434\n",
    "Tue June 2 21:43:43 2020 Epoch 30, lr: 0.0000009, train loss: 0.06801, val loss: 0.21014, acc: 73.11676, qwk: 0.90468\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

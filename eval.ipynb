{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models as tv_models\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import models\n",
    "import threading\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import re\n",
    "import gc\n",
    "import importlib\n",
    "import time\n",
    "import csv\n",
    "import sklearn.preprocessing\n",
    "import utils\n",
    "from sklearn.utils import class_weight\n",
    "import imagesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add configuration file\n",
    "# Dictionary for model configuration\n",
    "mdlParams = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import machine config\n",
    "pc_cfg = importlib.import_module('pc_cfgs.'+sys.argv[1])\n",
    "mdlParams.update(pc_cfg.mdlParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is another argument, its which checkpoint should be used\n",
    "if len(sys.argv) > 6:\n",
    "    if 'last' in sys.argv[6]:\n",
    "        mdlParams['ckpt_name'] = 'checkpoint-'\n",
    "    else:\n",
    "        mdlParams['ckpt_name'] = 'checkpoint_best-'\n",
    "    if 'first' in sys.argv[6]:\n",
    "        mdlParams['use_first'] = True\n",
    "else:\n",
    "    mdlParams['ckpt_name'] = 'checkpoint-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visible devices\n",
    "mdlParams['numGPUs']= [[int(s) for s in re.findall(r'\\d+',sys.argv[6])][-1]]\n",
    "cuda_str = \"\"\n",
    "for i in range(len(mdlParams['numGPUs'])):\n",
    "    cuda_str = cuda_str + str(mdlParams['numGPUs'][i])\n",
    "    if i is not len(mdlParams['numGPUs'])-1:\n",
    "        cuda_str = cuda_str + \",\"\n",
    "print(\"Devices to use:\",cuda_str)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cuda_str      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is another argument, also use a meta learner\n",
    "if len(sys.argv) > 7:\n",
    "    if 'HAMONLY' in sys.argv[7]:\n",
    "        mdlParams['eval_on_ham_only'] = True        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import model config\n",
    "model_cfg = importlib.import_module('cfgs.'+sys.argv[2])\n",
    "mdlParams_model = model_cfg.init(mdlParams)\n",
    "mdlParams.update(mdlParams_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path name where model is saved is the fourth argument\n",
    "if 'NONE' in sys.argv[5]:\n",
    "    mdlParams['saveDirBase'] = mdlParams['saveDir'] + sys.argv[2]\n",
    "else:\n",
    "    mdlParams['saveDirBase'] = sys.argv[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third is multi crop yes no\n",
    "if 'multi' in sys.argv[3]:\n",
    "    if 'rand' in sys.argv[3]:\n",
    "        mdlParams['numRandValSeq'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][0]\n",
    "        print(\"Random sequence number\",mdlParams['numRandValSeq'])\n",
    "    else:\n",
    "        mdlParams['numRandValSeq'] = 0\n",
    "    mdlParams['multiCropEval'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-1]\n",
    "    mdlParams['voting_scheme'] = sys.argv[4]\n",
    "    if 'scale' in sys.argv[3]:\n",
    "        print(\"Multi Crop and Scale Eval with crop number:\",mdlParams['multiCropEval'],\" Voting scheme: \",mdlParams['voting_scheme'])\n",
    "        mdlParams['orderedCrop'] = False\n",
    "        mdlParams['scale_min'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-2]/100.0\n",
    "    elif 'determ' in sys.argv[3]:\n",
    "        # Example application: multideterm5sc3f2\n",
    "        mdlParams['deterministic_eval'] = True\n",
    "        mdlParams['numCropPositions'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-3]\n",
    "        num_scales = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-2]\n",
    "        all_scales = [1.0,0.5,0.75,0.25,0.9,0.6,0.4]\n",
    "        mdlParams['cropScales'] = all_scales[:num_scales]\n",
    "        mdlParams['cropFlipping'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-1]\n",
    "        print(\"deterministic eval with crops number\",mdlParams['numCropPositions'],\"scales\",mdlParams['cropScales'],\"flipping\",mdlParams['cropFlipping'])\n",
    "        mdlParams['multiCropEval'] = mdlParams['numCropPositions']*len(mdlParams['cropScales'])*mdlParams['cropFlipping']\n",
    "        mdlParams['offset_crop'] = 0.2\n",
    "    elif 'order' in sys.argv[3]:\n",
    "        mdlParams['orderedCrop'] = True\n",
    "        if mdlParams.get('var_im_size',False):\n",
    "            # Crop positions, always choose multiCropEval to be 4, 9, 16, 25, etc.\n",
    "            mdlParams['cropPositions'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "            #mdlParams['imSizes'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "            for u in range(len(mdlParams['im_paths'])):\n",
    "                height, width = imagesize.get(mdlParams['im_paths'][u])\n",
    "                if width < mdlParams['input_size'][0]:\n",
    "                    height = int(mdlParams['input_size'][0]/float(width))*height\n",
    "                    width = mdlParams['input_size'][0]\n",
    "                if height < mdlParams['input_size'][0]:\n",
    "                    width = int(mdlParams['input_size'][0]/float(height))*width\n",
    "                    height = mdlParams['input_size'][0]     \n",
    "                if mdlParams.get('resize_large_ones') is not None:\n",
    "                    if width == mdlParams['large_size'] and height == mdlParams['large_size']:\n",
    "                        width, height = (mdlParams['resize_large_ones'],mdlParams['resize_large_ones'])                \n",
    "                ind = 0\n",
    "                for i in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "                    for j in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "                        mdlParams['cropPositions'][u,ind,0] = mdlParams['input_size'][0]/2+i*((width-mdlParams['input_size'][1])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                        mdlParams['cropPositions'][u,ind,1] = mdlParams['input_size'][1]/2+j*((height-mdlParams['input_size'][0])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                        #mdlParams['imSizes'][u,ind,0] = curr_im_size[0]\n",
    "\n",
    "                        ind += 1\n",
    "            # Sanity checks\n",
    "            #print(\"Positions\",mdlParams['cropPositions'])\n",
    "            # Test image sizes\n",
    "            height = mdlParams['input_size'][0]\n",
    "            width = mdlParams['input_size'][1]\n",
    "            for u in range(len(mdlParams['im_paths'])):                     \n",
    "                height_test, width_test = imagesize.get(mdlParams['im_paths'][u])\n",
    "                if width_test < mdlParams['input_size'][0]:\n",
    "                    height_test = int(mdlParams['input_size'][0]/float(width_test))*height_test\n",
    "                    width_test = mdlParams['input_size'][0]\n",
    "                if height_test < mdlParams['input_size'][0]:\n",
    "                    width_test = int(mdlParams['input_size'][0]/float(height_test))*width_test\n",
    "                    height_test = mdlParams['input_size'][0]     \n",
    "                if mdlParams.get('resize_large_ones') is not None:\n",
    "                    if width_test == mdlParams['large_size'] and height_test == mdlParams['large_size']:\n",
    "                        width_test, height_test = (mdlParams['resize_large_ones'],mdlParams['resize_large_ones'])                                   \n",
    "                test_im = np.zeros([width_test,height_test]) \n",
    "                for i in range(mdlParams['multiCropEval']):\n",
    "                    im_crop = test_im[np.int32(mdlParams['cropPositions'][u,i,0]-height/2):np.int32(mdlParams['cropPositions'][u,i,0]-height/2)+height,np.int32(mdlParams['cropPositions'][u,i,1]-width/2):np.int32(mdlParams['cropPositions'][u,i,1]-width/2)+width]\n",
    "                    if im_crop.shape[0] != mdlParams['input_size'][0]:\n",
    "                        print(\"Wrong shape\",im_crop.shape[0],mdlParams['im_paths'][u])    \n",
    "                    if im_crop.shape[1] != mdlParams['input_size'][1]:\n",
    "                        print(\"Wrong shape\",im_crop.shape[1],mdlParams['im_paths'][u]) \n",
    "        else:\n",
    "            # Crop positions, always choose multiCropEval to be 4, 9, 16, 25, etc.\n",
    "            mdlParams['cropPositions'] = np.zeros([mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "            if mdlParams['multiCropEval'] == 5:\n",
    "                numCrops = 4\n",
    "            elif mdlParams['multiCropEval'] == 7:\n",
    "                numCrops = 9\n",
    "                mdlParams['cropPositions'] = np.zeros([9,2],dtype=np.int64)\n",
    "            else:\n",
    "                numCrops = mdlParams['multiCropEval']\n",
    "            ind = 0\n",
    "            for i in range(np.int32(np.sqrt(numCrops))):\n",
    "                for j in range(np.int32(np.sqrt(numCrops))):\n",
    "                    mdlParams['cropPositions'][ind,0] = mdlParams['input_size'][0]/2+i*((mdlParams['input_size_load'][0]-mdlParams['input_size'][0])/(np.sqrt(numCrops)-1))\n",
    "                    mdlParams['cropPositions'][ind,1] = mdlParams['input_size'][1]/2+j*((mdlParams['input_size_load'][1]-mdlParams['input_size'][1])/(np.sqrt(numCrops)-1))\n",
    "                    ind += 1\n",
    "            # Add center crop\n",
    "            if mdlParams['multiCropEval'] == 5:\n",
    "                mdlParams['cropPositions'][4,0] = mdlParams['input_size_load'][0]/2\n",
    "                mdlParams['cropPositions'][4,1] = mdlParams['input_size_load'][1]/2   \n",
    "            if mdlParams['multiCropEval'] == 7:      \n",
    "                mdlParams['cropPositions'] = np.delete(mdlParams['cropPositions'],[3,7],0)                     \n",
    "            # Sanity checks\n",
    "            print(\"Positions val\",mdlParams['cropPositions'])\n",
    "            # Test image sizes\n",
    "            test_im = np.zeros(mdlParams['input_size_load'])\n",
    "            height = mdlParams['input_size'][0]\n",
    "            width = mdlParams['input_size'][1]\n",
    "            for i in range(mdlParams['multiCropEval']):\n",
    "                im_crop = test_im[np.int32(mdlParams['cropPositions'][i,0]-height/2):np.int32(mdlParams['cropPositions'][i,0]-height/2)+height,np.int32(mdlParams['cropPositions'][i,1]-width/2):np.int32(mdlParams['cropPositions'][i,1]-width/2)+width,:]\n",
    "                print(\"Shape\",i+1,im_crop.shape)         \n",
    "        print(\"Multi Crop with order with crop number:\",mdlParams['multiCropEval'],\" Voting scheme: \",mdlParams['voting_scheme'])\n",
    "        if 'flip' in sys.argv[3]:\n",
    "            # additional flipping, example: flip2multiorder16\n",
    "            mdlParams['eval_flipping'] = [int(s) for s in re.findall(r'\\d+',sys.argv[3])][-2]\n",
    "            print(\"Additional flipping\",mdlParams['eval_flipping'])\n",
    "    else:\n",
    "        print(\"Multi Crop Eval with crop number:\",mdlParams['multiCropEval'],\" Voting scheme: \",mdlParams['voting_scheme'])\n",
    "        mdlParams['orderedCrop'] = False\n",
    "else:\n",
    "    mdlParams['multiCropEval'] = 0\n",
    "    mdlParams['orderedCrop'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training set to eval mode\n",
    "mdlParams['trainSetState'] = 'eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mdlParams['numClasses'] == 9 and mdlParams.get('no_c9_eval',False):\n",
    "    num_classes = mdlParams['numClasses']-1    \n",
    "else:\n",
    "    num_classes = mdlParams['numClasses']\n",
    "# Save results in here\n",
    "allData = {}\n",
    "allData['f1Best'] = np.zeros([mdlParams['numCV']])\n",
    "allData['sensBest'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['specBest'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['accBest'] = np.zeros([mdlParams['numCV']])\n",
    "allData['waccBest'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['aucBest'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['convergeTime'] = {}\n",
    "allData['bestPred'] = {}\n",
    "allData['bestPredMC'] = {}\n",
    "allData['targets'] = {}\n",
    "allData['extPred'] = {}\n",
    "allData['f1Best_meta'] = np.zeros([mdlParams['numCV']])\n",
    "allData['sensBest_meta'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['specBest_meta'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['accBest_meta'] = np.zeros([mdlParams['numCV']])\n",
    "allData['waccBest_meta'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "allData['aucBest_meta'] = np.zeros([mdlParams['numCV'],num_classes])\n",
    "#allData['convergeTime'] = {}\n",
    "allData['bestPred_meta'] = {}\n",
    "allData['targets_meta'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (len(sys.argv) > 8):\n",
    "    for cv in range(mdlParams['numCV']):\n",
    "        # Reset model graph \n",
    "        importlib.reload(models)\n",
    "        #importlib.reload(torchvision)\n",
    "        # Collect model variables\n",
    "        modelVars = {}\n",
    "        modelVars['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(modelVars['device'])\n",
    "        # Def current CV set\n",
    "        mdlParams['trainInd'] = mdlParams['trainIndCV'][cv]\n",
    "        if 'valIndCV' in mdlParams:\n",
    "            mdlParams['valInd'] = mdlParams['valIndCV'][cv]\n",
    "        # Def current path for saving stuff\n",
    "        if 'valIndCV' in mdlParams:\n",
    "            mdlParams['saveDir'] = mdlParams['saveDirBase'] + '/CVSet' + str(cv)\n",
    "        else:\n",
    "            mdlParams['saveDir'] = mdlParams['saveDirBase']\n",
    "\n",
    "        # Potentially calculate setMean to subtract\n",
    "        if mdlParams['subtract_set_mean'] == 1:\n",
    "            mdlParams['setMean'] = np.mean(mdlParams['images_means'][mdlParams['trainInd'],:],(0))\n",
    "            print(\"Set Mean\",mdlParams['setMean']) \n",
    "\n",
    "        # Potentially only HAM eval\n",
    "        if mdlParams.get('eval_on_ham_only',False):\n",
    "            print(\"Old val inds\",len(mdlParams['valInd']))\n",
    "            mdlParams['valInd'] = np.intersect1d(mdlParams['valInd'],mdlParams['HAM10000_inds'])\n",
    "            print(\"New val inds, HAM only\",len(mdlParams['valInd']))\n",
    "\n",
    "        # balance classes\n",
    "        if mdlParams['balance_classes'] < 3 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 11:\n",
    "            class_weights = class_weight.compute_class_weight('balanced',np.unique(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)),np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)) \n",
    "            print(\"Current class weights\",class_weights)\n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "            print(\"Current class weights with extra\",class_weights)             \n",
    "        elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 4:\n",
    "            # Split training set by classes\n",
    "            not_one_hot = np.argmax(mdlParams['labels_array'],1)\n",
    "            mdlParams['class_indices'] = []\n",
    "            for i in range(mdlParams['numClasses']):\n",
    "                mdlParams['class_indices'].append(np.where(not_one_hot==i)[0])\n",
    "                # Kick out non-trainind indices\n",
    "                mdlParams['class_indices'][i] = np.setdiff1d(mdlParams['class_indices'][i],mdlParams['valInd'])\n",
    "                #print(\"Class\",i,mdlParams['class_indices'][i].shape,np.min(mdlParams['class_indices'][i]),np.max(mdlParams['class_indices'][i]),np.sum(mdlParams['labels_array'][np.int64(mdlParams['class_indices'][i]),:],0))        \n",
    "        elif mdlParams['balance_classes'] == 5 or mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 13:\n",
    "            # Other class balancing loss\n",
    "            class_weights = 1.0/np.mean(mdlParams['labels_array'][mdlParams['trainInd'],:],axis=0)\n",
    "            print(\"Current class weights\",class_weights) \n",
    "            class_weights = class_weights*mdlParams['extra_fac']\n",
    "            print(\"Current class weights with extra\",class_weights) \n",
    "        elif mdlParams['balance_classes'] == 9:\n",
    "            # Only use HAM indicies for calculation\n",
    "            print(\"Balance 9\")\n",
    "            indices_ham = mdlParams['trainInd'][mdlParams['trainInd'] < 25331]\n",
    "            if mdlParams['numClasses'] == 9:\n",
    "                class_weights_ = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:8],axis=0)\n",
    "                #print(\"class before\",class_weights_)\n",
    "                class_weights = np.zeros([mdlParams['numClasses']])\n",
    "                class_weights[:8] = class_weights_\n",
    "                class_weights[-1] = np.max(class_weights_)\n",
    "            else:\n",
    "                class_weights = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:],axis=0)\n",
    "            print(\"Current class weights\",class_weights)             \n",
    "            if isinstance(mdlParams['extra_fac'], float):\n",
    "                class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "            else:\n",
    "                class_weights = class_weights*mdlParams['extra_fac']\n",
    "            print(\"Current class weights with extra\",class_weights) \n",
    "\n",
    "\n",
    "        # Set up dataloaders\n",
    "        # Meta scaler\n",
    "        if mdlParams.get('meta_features',None) is not None and mdlParams['scale_features']:\n",
    "            mdlParams['feature_scaler_meta'] = sklearn.preprocessing.StandardScaler().fit(mdlParams['meta_array'][mdlParams['trainInd'],:])  \n",
    "            #print(\"scaler mean\",mdlParams['feature_scaler_meta'].mean_,\"var\",mdlParams['feature_scaler_meta'].var_)  \n",
    "        # For train\n",
    "        dataset_train = utils.ISICDataset(mdlParams, 'trainInd')\n",
    "        # For val\n",
    "        dataset_val = utils.ISICDataset(mdlParams, 'valInd')\n",
    "        if mdlParams['multiCropEval'] > 0:\n",
    "            modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['multiCropEval'], shuffle=False, num_workers=8, pin_memory=True)  \n",
    "        else:\n",
    "            modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['batchSize'], shuffle=False, num_workers=8, pin_memory=True)         \n",
    "         \n",
    "        modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], shuffle=True, num_workers=8, pin_memory=True)\n",
    "    \n",
    "        # For test\n",
    "        if 'testInd' in mdlParams:\n",
    "            dataset_test = utils.ISICDataset(mdlParams, 'testInd')\n",
    "            if mdlParams['multiCropEval'] > 0:\n",
    "                modelVars['dataloader_testInd'] = DataLoader(dataset_test, batch_size=mdlParams['multiCropEval'], shuffle=False, num_workers=8, pin_memory=True)  \n",
    "            else:\n",
    "                modelVars['dataloader_testInd'] = DataLoader(dataset_test, batch_size=mdlParams['batchSize'], shuffle=False, num_workers=8, pin_memory=True)            \n",
    "           \n",
    "            \n",
    "        modelVars['model'] = models.getModel(mdlParams)()\n",
    "        # Original input size\n",
    "        #if 'Dense' not in mdlParams['model_type']:\n",
    "        #    print(\"Original input size\",modelVars['model'].input_size)\n",
    "        #print(modelVars['model'])\n",
    "        if 'Dense' in mdlParams['model_type']:\n",
    "            if mdlParams['input_size'][0] != 224:\n",
    "                modelVars['model'] = utils.modify_densenet_avg_pool(modelVars['model'])\n",
    "                #print(modelVars['model'])\n",
    "            num_ftrs = modelVars['model'].classifier.in_features\n",
    "            modelVars['model'].classifier = nn.Linear(num_ftrs, mdlParams['numClasses'])\n",
    "            #print(modelVars['model'])\n",
    "        elif 'dpn' in mdlParams['model_type']:\n",
    "            num_ftrs = modelVars['model'].classifier.in_channels\n",
    "            modelVars['model'].classifier = nn.Conv2d(num_ftrs,mdlParams['numClasses'],[1,1])\n",
    "            #modelVars['model'].add_module('real_classifier',nn.Linear(num_ftrs, mdlParams['numClasses']))\n",
    "            #print(modelVars['model'])\n",
    "        elif 'efficient' in mdlParams['model_type']:\n",
    "            # Do nothing, output is prepared\n",
    "            num_ftrs = modelVars['model']._fc.in_features\n",
    "            modelVars['model']._fc = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "        elif 'wsl' in mdlParams['model_type']:\n",
    "            num_ftrs = modelVars['model'].fc.in_features\n",
    "            modelVars['model'].fc = nn.Linear(num_ftrs, mdlParams['numClasses'])          \n",
    "        else:\n",
    "            num_ftrs = modelVars['model'].last_linear.in_features\n",
    "            modelVars['model'].last_linear = nn.Linear(num_ftrs, mdlParams['numClasses'])   \n",
    "        # modify model\n",
    "        if mdlParams.get('meta_features',None) is not None:\n",
    "            modelVars['model'] = models.modify_meta(mdlParams,modelVars['model'])               \n",
    "        modelVars['model']  = modelVars['model'].to(modelVars['device'])\n",
    "        #summary(modelVars['model'], (mdlParams['input_size'][2], mdlParams['input_size'][0], mdlParams['input_size'][1]))\n",
    "        # Loss, with class weighting\n",
    "        # Loss, with class weighting\n",
    "        if mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 0 or mdlParams['balance_classes'] == 12:\n",
    "            modelVars['criterion'] = nn.CrossEntropyLoss()\n",
    "        elif mdlParams['balance_classes'] == 8:\n",
    "            modelVars['criterion'] = nn.CrossEntropyLoss(reduce=False)\n",
    "        elif mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7:\n",
    "            modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)),reduce=False)\n",
    "        elif mdlParams['balance_classes'] == 10:\n",
    "            modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'])\n",
    "        elif mdlParams['balance_classes'] == 11:\n",
    "            modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'],alpha=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "        else:\n",
    "            modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        modelVars['optimizer'] = optim.Adam(modelVars['model'].parameters(), lr=mdlParams['learning_rate'])\n",
    "\n",
    "        # Decay LR by a factor of 0.1 every 7 epochs\n",
    "        modelVars['scheduler'] = lr_scheduler.StepLR(modelVars['optimizer'], step_size=mdlParams['lowerLRAfter'], gamma=1/np.float32(mdlParams['LRstep']))\n",
    "\n",
    "        # Define softmax\n",
    "        modelVars['softmax'] = nn.Softmax(dim=1)\n",
    "\n",
    "        # Manually find latest chekcpoint, tf.train.latest_checkpoint is doing weird shit\n",
    "        files = glob(mdlParams['saveDir']+'/*')\n",
    "        #print(mdlParams['saveDir'])\n",
    "        #print(\"Files\",files)\n",
    "        global_steps = np.zeros([len(files)])\n",
    "        for i in range(len(files)):\n",
    "            # Use meta files to find the highest index\n",
    "            if 'checkpoint' not in files[i]:\n",
    "                continue\n",
    "            if mdlParams['ckpt_name'] not in files[i]:\n",
    "                continue\n",
    "            # Extract global step\n",
    "            nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "            global_steps[i] = nums[-1]\n",
    "        # Create path with maximum global step found, if first is not wanted\n",
    "        global_steps = np.sort(global_steps)\n",
    "        if mdlParams.get('use_first') is not None:\n",
    "            chkPath = mdlParams['saveDir'] + '/' + mdlParams['ckpt_name'] + str(int(global_steps[-2])) + '.pt'\n",
    "        else:\n",
    "            chkPath = mdlParams['saveDir'] + '/' + mdlParams['ckpt_name'] + str(int(np.max(global_steps))) + '.pt'\n",
    "        print(\"Restoring: \",chkPath)\n",
    "        # Load\n",
    "        state = torch.load(chkPath)\n",
    "        # Initialize model and optimizer\n",
    "        modelVars['model'].load_state_dict(state['state_dict'])\n",
    "        #modelVars['optimizer'].load_state_dict(state['optimizer'])   \n",
    "        # Construct pkl filename: config name, last/best, saved epoch number\n",
    "        pklFileName = sys.argv[2] + \"_\" + sys.argv[6] + \"_\" + str(int(np.max(global_steps))) + \".pkl\"\n",
    "        modelVars['model'].eval()\n",
    "        if mdlParams['classification']:\n",
    "            print(\"CV Set \",cv+1)\n",
    "            print(\"------------------------------------\")\n",
    "            # Training err first, deactivated\n",
    "            if 'trainInd' in mdlParams and False:\n",
    "                loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, _ = utils.getErrClassification_mgpu(mdlParams, 'trainInd', modelVars)\n",
    "                print(\"Training Results:\")\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"Loss\",np.mean(loss))\n",
    "                print(\"F1 Score\",f1)            \n",
    "                print(\"Sensitivity\",sensitivity)\n",
    "                print(\"Specificity\",specificity)\n",
    "                print(\"Accuracy\",accuracy)\n",
    "                print(\"Per Class Accuracy\",waccuracy)\n",
    "                print(\"Weighted Accuracy\",waccuracy)\n",
    "                print(\"AUC\",auc)\n",
    "                print(\"Mean AUC\", np.mean(auc))            \n",
    "            if 'valInd' in mdlParams and (len(sys.argv) <= 8):\n",
    "                loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, predictions_mc = utils.getErrClassification_mgpu(mdlParams, 'valInd', modelVars)\n",
    "                print(\"Validation Results:\")\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"Loss\",np.mean(loss))\n",
    "                print(\"F1 Score\",f1)            \n",
    "                print(\"Sensitivity\",sensitivity)\n",
    "                print(\"Specificity\",specificity)\n",
    "                print(\"Accuracy\",accuracy)\n",
    "                print(\"Per Class Accuracy\",waccuracy)\n",
    "                print(\"Weighted Accuracy\",np.mean(waccuracy))\n",
    "                print(\"AUC\",auc)\n",
    "                print(\"Mean AUC\", np.mean(auc))  \n",
    "                # Save results in dict\n",
    "                if 'testInd' not in mdlParams:\n",
    "                    allData['f1Best'][cv] = f1\n",
    "                    allData['sensBest'][cv,:] = sensitivity\n",
    "                    allData['specBest'][cv,:] = specificity\n",
    "                    allData['accBest'][cv] = accuracy\n",
    "                    allData['waccBest'][cv,:] = waccuracy\n",
    "                    allData['aucBest'][cv,:] = auc  \n",
    "                allData['bestPred'][cv] = predictions\n",
    "                allData['bestPredMC'][cv] = predictions_mc\n",
    "                allData['targets'][cv] = targets \n",
    "                print(\"Pred shape\",predictions.shape,\"Tar shape\",targets.shape)\n",
    "            if 'testInd' in mdlParams:        \n",
    "                loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, predictions_mc = utils.getErrClassification_mgpu(mdlParams, 'testInd', modelVars)\n",
    "                print(\"Test Results Normal:\")\n",
    "                print(\"----------------------------------\")\n",
    "                print(\"Loss\",np.mean(loss))\n",
    "                print(\"F1 Score\",f1)            \n",
    "                print(\"Sensitivity\",sensitivity)\n",
    "                print(\"Specificity\",specificity)\n",
    "                print(\"Accuracy\",accuracy)\n",
    "                print(\"Per Class Accuracy\",waccuracy)\n",
    "                print(\"Weighted Accuracy\",np.mean(waccuracy))\n",
    "                print(\"AUC\",auc)\n",
    "                print(\"Mean AUC\", np.mean(auc))  \n",
    "                # Save results in dict\n",
    "                allData['f1Best'][cv] = f1\n",
    "                allData['sensBest'][cv,:] = sensitivity\n",
    "                allData['specBest'][cv,:] = specificity\n",
    "                allData['accBest'][cv] = accuracy\n",
    "                allData['waccBest'][cv,:] = waccuracy\n",
    "                allData['aucBest'][cv,:] = auc    \n",
    "        else:\n",
    "            # TODO: Regression\n",
    "            print(\"Not Implemented\")            \n",
    "# If there is an 8th argument, make extra evaluation for external set\n",
    "if len(sys.argv) > 8:\n",
    "    for cv in range(mdlParams['numCV']):\n",
    "            # Reset model graph \n",
    "            importlib.reload(models)\n",
    "            #importlib.reload(torchvision)\n",
    "            # Collect model variables\n",
    "            modelVars = {}\n",
    "            modelVars['device'] = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")    \n",
    "            # define new folder, take care that there might be no labels\n",
    "            print(\"Creating predictions for path \",sys.argv[8])\n",
    "            # Add meta data\n",
    "            if mdlParams.get('meta_features',None) is not None:\n",
    "                mdlParams['meta_dict'] = {}\n",
    "                path1 = mdlParams['dataDir'] + '/meta_data/test_rez3_ll/meta_data_test.pkl'\n",
    "                # Open and load\n",
    "                with open(path1,'rb') as f:\n",
    "                    meta_data = pickle.load(f)\n",
    "                # Write into dict\n",
    "                for k in range(len(meta_data['im_name'])):\n",
    "                    feature_vector = []\n",
    "                    if 'age_oh' in mdlParams['meta_features']:\n",
    "                        if mdlParams['encode_nan']:\n",
    "                            feature_vector.append(meta_data['age_oh'][k,:])\n",
    "                        else:\n",
    "                            feature_vector.append(meta_data['age_oh'][k,1:])\n",
    "                    if 'age_num' in mdlParams['meta_features']:\n",
    "                        feature_vector.append(np.array([meta_data['age_num'][k]]))                      \n",
    "                    if 'loc_oh' in mdlParams['meta_features']:\n",
    "                        if mdlParams['encode_nan']:\n",
    "                            feature_vector.append(meta_data['loc_oh'][k,:])\n",
    "                        else:\n",
    "                            feature_vector.append(meta_data['loc_oh'][k,1:])\n",
    "                    if 'sex_oh' in mdlParams['meta_features']:\n",
    "                        if mdlParams['encode_nan']:\n",
    "                            feature_vector.append(meta_data['sex_oh'][k,:])\n",
    "                        else:\n",
    "                            feature_vector.append(meta_data['sex_oh'][k,1:]) \n",
    "\n",
    "                    #print(feature_vector) \n",
    "                    feature_vector = np.concatenate(feature_vector,axis=0)\n",
    "                    #print(\"feature vector shape\",feature_vector.shape)                                                \n",
    "                    mdlParams['meta_dict'][meta_data['im_name'][k]] = feature_vector                  \n",
    "            # Define the path\n",
    "            path1 = sys.argv[8]\n",
    "            # All files in that set\n",
    "            files = sorted(glob(path1+'/*'))\n",
    "            # Define new paths\n",
    "            mdlParams['im_paths'] = []\n",
    "            mdlParams['meta_list'] = []\n",
    "            for j in range(len(files)):\n",
    "                inds = [int(s) for s in re.findall(r'\\d+',files[j])]\n",
    "                if 'ISIC_' in files[j]:\n",
    "                    mdlParams['im_paths'].append(files[j])\n",
    "                    if mdlParams.get('meta_features',None) is not None:\n",
    "                        for key in mdlParams['meta_dict']:\n",
    "                            if key in files[j]:\n",
    "                                mdlParams['meta_list'].append(mdlParams['meta_dict'][key])       \n",
    "            if mdlParams.get('meta_features',None) is not None:\n",
    "                # Meta data\n",
    "                mdlParams['meta_array'] = np.array(mdlParams['meta_list'])                \n",
    "            # Add empty labels\n",
    "            mdlParams['labels_array'] = np.zeros([len(mdlParams['im_paths']),mdlParams['numClasses']],dtype=np.float32)\n",
    "            # Define everything as a valind set\n",
    "            mdlParams['valInd'] = np.array(np.arange(len(mdlParams['im_paths'])))\n",
    "            mdlParams['trainInd'] = mdlParams['valInd']\n",
    "            if mdlParams.get('var_im_size',False):\n",
    "                # Crop positions, always choose multiCropEval to be 4, 9, 16, 25, etc.\n",
    "                mdlParams['cropPositions'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "                #mdlParams['imSizes'] = np.zeros([len(mdlParams['im_paths']),mdlParams['multiCropEval'],2],dtype=np.int64)\n",
    "                for u in range(len(mdlParams['im_paths'])):\n",
    "                    height, width = imagesize.get(mdlParams['im_paths'][u])\n",
    "                    if width < mdlParams['input_size'][0]:\n",
    "                        height = int(mdlParams['input_size'][0]/float(width))*height\n",
    "                        width = mdlParams['input_size'][0]\n",
    "                    if height < mdlParams['input_size'][0]:\n",
    "                        width = int(mdlParams['input_size'][0]/float(height))*width\n",
    "                        height = mdlParams['input_size'][0]     \n",
    "                    if mdlParams.get('resize_large_ones') is not None:\n",
    "                        if width == mdlParams['large_size'] and height == mdlParams['large_size']:\n",
    "                            width, height = (mdlParams['resize_large_ones'],mdlParams['resize_large_ones'])                \n",
    "                    ind = 0\n",
    "                    for i in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "                        for j in range(np.int32(np.sqrt(mdlParams['multiCropEval']))):\n",
    "                            mdlParams['cropPositions'][u,ind,0] = mdlParams['input_size'][0]/2+i*((width-mdlParams['input_size'][1])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                            mdlParams['cropPositions'][u,ind,1] = mdlParams['input_size'][1]/2+j*((height-mdlParams['input_size'][0])/(np.sqrt(mdlParams['multiCropEval'])-1))\n",
    "                            #mdlParams['imSizes'][u,ind,0] = curr_im_size[0]\n",
    "\n",
    "                            ind += 1\n",
    "                # Sanity checks\n",
    "                #print(\"Positions\",mdlParams['cropPositions'])\n",
    "                # Test image sizes\n",
    "                test_im = np.zeros(mdlParams['input_size_load'])\n",
    "                height = mdlParams['input_size'][0]\n",
    "                width = mdlParams['input_size'][1]\n",
    "                for u in range(len(mdlParams['im_paths'])):                     \n",
    "                    height_test, width_test = imagesize.get(mdlParams['im_paths'][u])\n",
    "                    if width_test < mdlParams['input_size'][0]:\n",
    "                        height_test = int(mdlParams['input_size'][0]/float(width_test))*height_test\n",
    "                        width_test = mdlParams['input_size'][0]\n",
    "                    if height_test < mdlParams['input_size'][0]:\n",
    "                        width_test = int(mdlParams['input_size'][0]/float(height_test))*width_test\n",
    "                        height_test = mdlParams['input_size'][0]     \n",
    "                    if mdlParams.get('resize_large_ones') is not None:\n",
    "                        if width_test == mdlParams['large_size'] and height_test == mdlParams['large_size']:\n",
    "                            width_test, height_test = (mdlParams['resize_large_ones'],mdlParams['resize_large_ones'])                                   \n",
    "                    test_im = np.zeros([width_test,height_test]) \n",
    "                    for i in range(mdlParams['multiCropEval']):\n",
    "                        im_crop = test_im[np.int32(mdlParams['cropPositions'][u,i,0]-height/2):np.int32(mdlParams['cropPositions'][u,i,0]-height/2)+height,np.int32(mdlParams['cropPositions'][u,i,1]-width/2):np.int32(mdlParams['cropPositions'][u,i,1]-width/2)+width]\n",
    "                        if im_crop.shape[0] != mdlParams['input_size'][0]:\n",
    "                            print(\"Wrong shape\",im_crop.shape[0],mdlParams['im_paths'][u])    \n",
    "                        if im_crop.shape[1] != mdlParams['input_size'][1]:\n",
    "                            print(\"Wrong shape\",im_crop.shape[1],mdlParams['im_paths'][u])                 \n",
    "            mdlParams['saveDir'] = mdlParams['saveDirBase'] + '/CVSet' + str(cv)\n",
    "            # balance classes\n",
    "            if mdlParams['balance_classes'] < 3 or mdlParams['balance_classes'] == 7 or mdlParams['balance_classes'] == 11:\n",
    "                class_weights = class_weight.compute_class_weight('balanced',np.unique(np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)),np.argmax(mdlParams['labels_array'][mdlParams['trainInd'],:],1)) \n",
    "                print(\"Current class weights\",class_weights)\n",
    "                class_weights = class_weights*mdlParams['extra_fac']\n",
    "                print(\"Current class weights with extra\",class_weights)             \n",
    "            elif mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 4:\n",
    "                # Split training set by classes\n",
    "                not_one_hot = np.argmax(mdlParams['labels_array'],1)\n",
    "                mdlParams['class_indices'] = []\n",
    "                for i in range(mdlParams['numClasses']):\n",
    "                    mdlParams['class_indices'].append(np.where(not_one_hot==i)[0])\n",
    "                    # Kick out non-trainind indices\n",
    "                    mdlParams['class_indices'][i] = np.setdiff1d(mdlParams['class_indices'][i],mdlParams['valInd'])\n",
    "                    #print(\"Class\",i,mdlParams['class_indices'][i].shape,np.min(mdlParams['class_indices'][i]),np.max(mdlParams['class_indices'][i]),np.sum(mdlParams['labels_array'][np.int64(mdlParams['class_indices'][i]),:],0))        \n",
    "            elif mdlParams['balance_classes'] == 5 or mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 13:\n",
    "                # Other class balancing loss\n",
    "                class_weights = 1.0/np.mean(mdlParams['labels_array'][mdlParams['trainInd'],:],axis=0)\n",
    "                print(\"Current class weights\",class_weights) \n",
    "                class_weights = class_weights*mdlParams['extra_fac']\n",
    "                print(\"Current class weights with extra\",class_weights) \n",
    "            elif mdlParams['balance_classes'] == 9:\n",
    "                # Only use official indicies for calculation\n",
    "                print(\"Balance 9\")\n",
    "                indices_ham = mdlParams['trainInd'][mdlParams['trainInd'] < 25331]\n",
    "                if mdlParams['numClasses'] == 9:\n",
    "                    class_weights_ = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:8],axis=0)\n",
    "                    #print(\"class before\",class_weights_)\n",
    "                    class_weights = np.zeros([mdlParams['numClasses']])\n",
    "                    class_weights[:8] = class_weights_\n",
    "                    class_weights[-1] = np.max(class_weights_)\n",
    "                else:\n",
    "                    class_weights = 1.0/np.mean(mdlParams['labels_array'][indices_ham,:],axis=0)\n",
    "                print(\"Current class weights\",class_weights)             \n",
    "                if isinstance(mdlParams['extra_fac'], float):\n",
    "                    class_weights = np.power(class_weights,mdlParams['extra_fac'])\n",
    "                else:\n",
    "                    class_weights = class_weights*mdlParams['extra_fac']\n",
    "                print(\"Current class weights with extra\",class_weights) \n",
    "\n",
    "\n",
    "            # Set up dataloaders\n",
    "            # Meta scaler\n",
    "            if mdlParams.get('meta_features',None) is not None and mdlParams['scale_features']:\n",
    "                mdlParams['feature_scaler_meta'] = sklearn.preprocessing.StandardScaler().fit(mdlParams['meta_array'][mdlParams['trainInd'],:])  \n",
    "                #print(\"scaler mean\",mdlParams['feature_scaler_meta'].mean_,\"var\",mdlParams['feature_scaler_meta'].var_)              \n",
    "            # For train\n",
    "            dataset_train = utils.ISICDataset(mdlParams, 'trainInd')\n",
    "            # For val\n",
    "            dataset_val = utils.ISICDataset(mdlParams, 'valInd')\n",
    "            if mdlParams['multiCropEval'] > 0:\n",
    "                modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['multiCropEval'], shuffle=False, num_workers=8, pin_memory=True)  \n",
    "            else:\n",
    "                modelVars['dataloader_valInd'] = DataLoader(dataset_val, batch_size=mdlParams['batchSize'], shuffle=False, num_workers=8, pin_memory=True)               \n",
    "            modelVars['dataloader_trainInd'] = DataLoader(dataset_train, batch_size=mdlParams['batchSize'], shuffle=True, num_workers=8, pin_memory=True)\n",
    "                    \n",
    "\n",
    "            # Define model \n",
    "            modelVars['model'] = models.getModel(mdlParams)()             \n",
    "            if 'Dense' in mdlParams['model_type']:\n",
    "                if mdlParams['input_size'][0] != 224:\n",
    "                    modelVars['model'] = utils.modify_densenet_avg_pool(modelVars['model'])\n",
    "                    #print(modelVars['model'])\n",
    "                num_ftrs = modelVars['model'].classifier.in_features\n",
    "                modelVars['model'].classifier = nn.Linear(num_ftrs, mdlParams['numClasses'])\n",
    "                #print(modelVars['model'])\n",
    "            elif 'dpn' in mdlParams['model_type']:\n",
    "                num_ftrs = modelVars['model'].classifier.in_channels\n",
    "                modelVars['model'].classifier = nn.Conv2d(num_ftrs,mdlParams['numClasses'],[1,1])\n",
    "                #modelVars['model'].add_module('real_classifier',nn.Linear(num_ftrs, mdlParams['numClasses']))\n",
    "                #print(modelVars['model'])\n",
    "            elif 'efficient' in mdlParams['model_type']:\n",
    "                # Do nothing, output is prepared\n",
    "                num_ftrs = modelVars['model']._fc.in_features\n",
    "                modelVars['model']._fc = nn.Linear(num_ftrs, mdlParams['numClasses'])    \n",
    "            elif 'wsl' in mdlParams['model_type']:\n",
    "                num_ftrs = modelVars['model'].fc.in_features\n",
    "                modelVars['model'].fc = nn.Linear(num_ftrs, mdlParams['numClasses'])          \n",
    "            else:\n",
    "                num_ftrs = modelVars['model'].last_linear.in_features\n",
    "                modelVars['model'].last_linear = nn.Linear(num_ftrs, mdlParams['numClasses'])   \n",
    "            # modify model\n",
    "            if mdlParams.get('meta_features',None) is not None:\n",
    "                modelVars['model'] = models.modify_meta(mdlParams,modelVars['model'])  \n",
    "            modelVars['model']  = modelVars['model'].to(modelVars['device'])\n",
    "            #summary(modelVars['model'], (mdlParams['input_size'][2], mdlParams['input_size'][0], mdlParams['input_size'][1]))\n",
    "            # Loss, with class weighting\n",
    "            # Loss, with class weighting\n",
    "            if mdlParams['balance_classes'] == 3 or mdlParams['balance_classes'] == 0 or mdlParams['balance_classes'] == 12:\n",
    "                modelVars['criterion'] = nn.CrossEntropyLoss()\n",
    "            elif mdlParams['balance_classes'] == 8:\n",
    "                modelVars['criterion'] = nn.CrossEntropyLoss(reduce=False)\n",
    "            elif mdlParams['balance_classes'] == 6 or mdlParams['balance_classes'] == 7:\n",
    "                modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)),reduce=False)\n",
    "            elif mdlParams['balance_classes'] == 10:\n",
    "                modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'])\n",
    "            elif mdlParams['balance_classes'] == 11:\n",
    "                modelVars['criterion'] = utils.FocalLoss(mdlParams['numClasses'],alpha=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "            else:\n",
    "                modelVars['criterion'] = nn.CrossEntropyLoss(weight=torch.cuda.FloatTensor(class_weights.astype(np.float32)))\n",
    "            # Observe that all parameters are being optimized\n",
    "            modelVars['optimizer'] = optim.Adam(modelVars['model'].parameters(), lr=mdlParams['learning_rate'])\n",
    "\n",
    "            # Decay LR by a factor of 0.1 every 7 epochs\n",
    "            modelVars['scheduler'] = lr_scheduler.StepLR(modelVars['optimizer'], step_size=mdlParams['lowerLRAfter'], gamma=1/np.float32(mdlParams['LRstep']))\n",
    "\n",
    "            # Define softmax\n",
    "            modelVars['softmax'] = nn.Softmax(dim=1)\n",
    "\n",
    "            # Manually find latest chekcpoint, tf.train.latest_checkpoint is doing weird shit\n",
    "            files = glob(mdlParams['saveDir']+'/*')\n",
    "            global_steps = np.zeros([len(files)])\n",
    "            for i in range(len(files)):\n",
    "                # Use meta files to find the highest index\n",
    "                if 'checkpoint' not in files[i]:\n",
    "                    continue\n",
    "                if mdlParams['ckpt_name'] not in files[i]:\n",
    "                    continue\n",
    "                # Extract global step\n",
    "                nums = [int(s) for s in re.findall(r'\\d+',files[i])]\n",
    "                global_steps[i] = nums[-1]\n",
    "            # Create path with maximum global step found, if first is not wanted\n",
    "            global_steps = np.sort(global_steps)\n",
    "            if mdlParams.get('use_first') is not None:\n",
    "                chkPath = mdlParams['saveDir'] + '/' + mdlParams['ckpt_name'] + str(int(global_steps[-2])) + '.pt'\n",
    "            else:\n",
    "                chkPath = mdlParams['saveDir'] + '/' + mdlParams['ckpt_name'] + str(int(np.max(global_steps))) + '.pt'\n",
    "            print(\"Restoring: \",chkPath)\n",
    "            \n",
    "            # Load\n",
    "            state = torch.load(chkPath)\n",
    "            # Initialize model and optimizer\n",
    "            modelVars['model'].load_state_dict(state['state_dict'])\n",
    "            #modelVars['optimizer'].load_state_dict(state['optimizer'])  \n",
    "            # Get predictions or learn on pred\n",
    "            modelVars['model'].eval()    \n",
    "            # Get predictions\n",
    "            # Turn off the skipping of the last class\n",
    "            mdlParams['no_c9_eval'] = False\n",
    "            loss, accuracy, sensitivity, specificity, conf_matrix, f1, auc, waccuracy, predictions, targets, predictions_mc = utils.getErrClassification_mgpu(mdlParams, 'valInd', modelVars)\n",
    "            # Save predictions            \n",
    "            allData['extPred'][cv] = predictions\n",
    "            print(\"extPred shape\",allData['extPred'][cv].shape)\n",
    "            pklFileName = sys.argv[2] + \"_\" + sys.argv[6] + \"_\" + str(int(np.max(global_steps))) + \"_predn.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean results over all folds\n",
    "np.set_printoptions(precision=4)\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"Mean over all Folds\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"F1 Score\",np.array([np.mean(allData['f1Best'])]),\"+-\",np.array([np.std(allData['f1Best'])]))       \n",
    "print(\"Sensitivtiy\",np.mean(allData['sensBest'],0),\"+-\",np.std(allData['sensBest'],0))  \n",
    "print(\"Specificity\",np.mean(allData['specBest'],0),\"+-\",np.std(allData['specBest'],0))  \n",
    "print(\"Mean Specificity\",np.array([np.mean(allData['specBest'])]),\"+-\",np.array([np.std(np.mean(allData['specBest'],1))]))  \n",
    "print(\"Accuracy\",np.array([np.mean(allData['accBest'])]),\"+-\",np.array([np.std(allData['accBest'])]))  \n",
    "print(\"Per Class Accuracy\",np.mean(allData['waccBest'],0),\"+-\",np.std(allData['waccBest'],0))\n",
    "print(\"Weighted Accuracy\",np.array([np.mean(allData['waccBest'])]),\"+-\",np.array([np.std(np.mean(allData['waccBest'],1))])) \n",
    "print(\"AUC\",np.mean(allData['aucBest'],0),\"+-\",np.std(allData['aucBest'],0))    \n",
    "print(\"Mean AUC\",np.array([np.mean(allData['aucBest'])]),\"+-\",np.array([np.std(np.mean(allData['aucBest'],1))]))      \n",
    "# Save dict with results\n",
    "with open(mdlParams['saveDirBase'] + \"/\" + pklFileName, 'wb') as f:\n",
    "    pickle.dump(allData, f, pickle.HIGHEST_PROTOCOL)              "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import h5py\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import scipy\n",
    "import pickle\n",
    "import imagesize\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from base import BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Base Path\n",
    "params['path_base'] = BASE_PATH\n",
    "## Save summaries and model here\n",
    "params['save_dir'] = params['path_base'] / 'out'\n",
    "## Data is loaded from here\n",
    "params['data_dir'] = params['path_base'] / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['model_type'] = 'efficientnet-b0'\n",
    "params['dataset_names'] = ['full']\n",
    "params['file_ending'] = '.jpg'\n",
    "params['exclude_inds'] = False\n",
    "params['same_sized_crops'] = False\n",
    "params['multiCropEval'] = 9\n",
    "params['var_im_size'] = False\n",
    "params['orderedCrop'] = False\n",
    "params['voting_scheme'] = 'average'    \n",
    "params['classification'] = True\n",
    "params['balance_classes'] = 2\n",
    "params['extra_fac'] = 1.0\n",
    "params['numClasses'] = 2\n",
    "params['no_c9_eval'] = True\n",
    "params['numOut'] = params['numClasses']\n",
    "params['numCV'] = 5\n",
    "params['trans_norm_first'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['deterministic_eval'] = True\n",
    "params['numCropPositions'] = 1\n",
    "num_scales = 4\n",
    "all_scales = [1.0,0.5,0.75,0.25,0.9,0.6,0.4]\n",
    "params['cropScales'] = all_scales[:num_scales]\n",
    "params['cropFlipping'] = 4\n",
    "params['multiCropEval'] = params['numCropPositions']*len(params['cropScales'])*params['cropFlipping']\n",
    "params['offset_crop'] = 0.2    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale up for b1-b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['input_size'] = [224,224,3]     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "comment_questions": false
   },
   "source": [
    "# Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "comment_questions": false
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "params['batchSize'] = 20#*len(params['numGPUs'])\n",
    "# Initial learning rate\n",
    "params['learning_rate'] = 0.001#*len(params['numGPUs'])\n",
    "# Lower learning rate after no improvement over 100 epochs\n",
    "params['lowerLRAfter'] = 25\n",
    "# If there is no validation set, start lowering the LR after X steps\n",
    "params['lowerLRat'] = 50\n",
    "# Divide learning rate by this value\n",
    "params['LRstep'] = 5\n",
    "# Maximum number of training iterations\n",
    "params['training_steps'] = 40 #250\n",
    "# Display error every X steps\n",
    "params['display_step'] = 1\n",
    "# Scale?\n",
    "params['scale_targets'] = False\n",
    "# Peak at test error during training? (generally, dont do this!)\n",
    "params['peak_at_testerr'] = False\n",
    "# Print trainerr\n",
    "params['print_trainerr'] = True\n",
    "# Subtract trainset mean?\n",
    "params['subtract_set_mean'] = False\n",
    "params['setMean'] = np.array([0.0, 0.0, 0.0])   \n",
    "params['setStd'] = np.array([1.0, 1.0, 1.0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data AUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params['full_color_distort'] = True\n",
    "params['autoaugment'] = False     \n",
    "params['flip_lr_ud'] = True\n",
    "params['full_rot'] = 180\n",
    "params['scale'] = (0.8,1.2)\n",
    "params['shear'] = 10\n",
    "params['cutout'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['meta_features'] = ['age_num','sex_oh','loc_oh']\n",
    "params['meta_feature_sizes'] = [1,8,2]\n",
    "params['encode_nan'] = False\n",
    "params['model_load_path'] = '/out/2020.test_effb0_rr'\n",
    "params['fc_layers_before'] = [256,256]\n",
    "params['fc_layers_after'] = [1024]\n",
    "params['freeze_cnn'] = True\n",
    "params['learning_rate_meta'] = 0.00001\n",
    "# each feature is set to missing with this prob\n",
    "params['drop_augment'] = 0.1\n",
    "params['dropout_meta'] = 0.4\n",
    "params['scale_features'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Check labels first \n",
    "img_name are the keys\n",
    "one-hot encoding targets as arrays are the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['preload'] = False\n",
    "params['labels_dict'] = {}\n",
    "# All sets\n",
    "all_sets = params['data_dir'] / 'labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through all sets\n",
    "for p in all_sets.iterdir():\n",
    "    if p.is_dir() and p.name in params['dataset_names']:\n",
    "        for file in p.iterdir():\n",
    "            if file.suffix == '.csv':\n",
    "                df = pd.read_csv(file)\n",
    "                keys = df.image_id.values\n",
    "                targets = df.drop('image_id', axis=1).values\n",
    "                params['labels_dict'].update(dict(zip(keys, targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all im paths here\n",
    "params['im_paths'] = []\n",
    "params['labels_list'] = []\n",
    "# Define the sets\n",
    "path1 = params['data_dir'] + '/images/'\n",
    "# All sets\n",
    "allSets = sorted(glob(path1 + '*/'))\n",
    "# Ids which name the folders\n",
    "# Make official first datasets\n",
    "for i in range(len(allSets)):\n",
    "    if params['dataset_names'][0] in allSets[i]:\n",
    "        temp = allSets[i]\n",
    "        allSets.remove(allSets[i])\n",
    "        allSets.insert(0, temp)\n",
    "print(allSets)        \n",
    "# Set of keys, for marking old HAM10000\n",
    "params['key_list'] = []\n",
    "if params['exclude_inds']:\n",
    "    with open(params['save_dir'] + 'indices_exclude.pkl','rb') as f:\n",
    "        indices_exclude = pickle.load(f)          \n",
    "    exclude_list = []    \n",
    "for i in range(len(allSets)):\n",
    "    # All files in that set\n",
    "    files = sorted(glob(allSets[i]+'*'))\n",
    "    # Check if there is something in there, if not, discard\n",
    "    if len(files) == 0:\n",
    "        continue\n",
    "    # Check if want to include this dataset\n",
    "    foundSet = False\n",
    "    for j in range(len(params['dataset_names'])):\n",
    "        if params['dataset_names'][j] in allSets[i]:\n",
    "            foundSet = True\n",
    "    if not foundSet:\n",
    "        continue                    \n",
    "    for j in tqdm(range(len(files))):\n",
    "        if '.jpg' in files[j] or '.jpeg' in files[j] or '.JPG' in files[j] or '.JPEG' in files[j] or '.png' in files[j] or '.PNG' in files[j]:                \n",
    "            # Add according label, find it first\n",
    "            found_already = False\n",
    "            for key in params['labels_dict']:\n",
    "                if key + params['file_ending'] in files[j]:\n",
    "                    if found_already:\n",
    "                        print(\"Found already:\",key,files[j])                     \n",
    "                    params['key_list'].append(key)\n",
    "                    params['labels_list'].append(params['labels_dict'][key])\n",
    "                    found_already = True\n",
    "            if found_already:\n",
    "                params['im_paths'].append(files[j])     \n",
    "                if params['exclude_inds']:\n",
    "                    for key in indices_exclude:\n",
    "                        if key in files[j]:\n",
    "                            exclude_list.append(indices_exclude[key])                                       \n",
    "# Convert label list to array\n",
    "params['labels_array'] = np.array(params['labels_list'])\n",
    "print(np.mean(params['labels_array'],axis=0))        \n",
    "# Create indices list with HAM10000 only\n",
    "params['HAM10000_inds'] = []\n",
    "HAM_START = 24306\n",
    "HAM_END = 34320\n",
    "for j in range(len(params['key_list'])):\n",
    "    try:\n",
    "        curr_id = [int(s) for s in re.findall(r'\\d+',params['key_list'][j])][-1]\n",
    "    except:\n",
    "        continue\n",
    "    if curr_id >= HAM_START and curr_id <= HAM_END:\n",
    "            params['HAM10000_inds'].append(j)\n",
    "    params['HAM10000_inds'] = np.array(params['HAM10000_inds'])    \n",
    "    print(\"Len ham\",len(params['HAM10000_inds']))   \n",
    "    # Perhaps preload images\n",
    "    if params['preload']:\n",
    "        params['images_array'] = np.zeros([len(params['im_paths']),params['input_size_load'][0],params['input_size_load'][1],params['input_size_load'][2]],dtype=np.uint8)\n",
    "        for i in range(len(params['im_paths'])):\n",
    "            x = scipy.ndimage.imread(params['im_paths'][i])\n",
    "            #x = x.astype(np.float32)   \n",
    "            # Scale to 0-1 \n",
    "            #min_x = np.min(x)\n",
    "            #max_x = np.max(x)\n",
    "            #x = (x-min_x)/(max_x-min_x)\n",
    "            params['images_array'][i,:,:,:] = x\n",
    "            if i%1000 == 0:\n",
    "                print(i+1,\"images loaded...\")     \n",
    "    if params['subtract_set_mean']:\n",
    "        params['images_means'] = np.zeros([len(params['im_paths']),3])\n",
    "        for i in range(len(params['im_paths'])):\n",
    "            x = scipy.ndimage.imread(params['im_paths'][i])\n",
    "            x = x.astype(np.float32)   \n",
    "            # Scale to 0-1 \n",
    "            min_x = np.min(x)\n",
    "            max_x = np.max(x)\n",
    "            x = (x-min_x)/(max_x-min_x)\n",
    "            params['images_means'][i,:] = np.mean(x,(0,1))\n",
    "            if i%1000 == 0:\n",
    "                print(i+1,\"images processed for mean...\")         \n",
    "\n",
    "    ### Define Indices ###\n",
    "    # Just divide into 5 equally large sets\n",
    "    with open(params['save_dir'] + 'indices_isic2020.pkl','rb') as f:\n",
    "        indices = pickle.load(f)           \n",
    "    params['trainIndCV'] = indices['trainIndCV']\n",
    "    params['valIndCV'] = indices['valIndCV']\n",
    "    if params['exclude_inds']:\n",
    "        exclude_list = np.array(exclude_list)\n",
    "        all_inds = np.arange(len(params['im_paths']))\n",
    "        exclude_inds = all_inds[exclude_list.astype(bool)]\n",
    "        for i in range(len(params['trainIndCV'])):\n",
    "            params['trainIndCV'][i] = np.setdiff1d(params['trainIndCV'][i],exclude_inds)\n",
    "        for i in range(len(params['valIndCV'])):\n",
    "            params['valIndCV'][i] = np.setdiff1d(params['valIndCV'][i],exclude_inds)     \n",
    "    # Consider case with more than one set\n",
    "    if len(params['dataset_names']) > 1:\n",
    "        restInds = np.array(np.arange(25331,params['labels_array'].shape[0]))\n",
    "        for i in range(params['numCV']):\n",
    "            params['trainIndCV'][i] = np.concatenate((params['trainIndCV'][i],restInds))        \n",
    "    print(\"Train\")\n",
    "    for i in range(len(params['trainIndCV'])):\n",
    "        print(params['trainIndCV'][i].shape)\n",
    "    print(\"Val\")\n",
    "    for i in range(len(params['valIndCV'])):\n",
    "        print(params['valIndCV'][i].shape)    \n",
    "\n",
    "    # Use this for ordered multi crops\n",
    "    if params['orderedCrop']:\n",
    "        # Crop positions, always choose multiCropEval to be 4, 9, 16, 25, etc.\n",
    "        params['cropPositions'] = np.zeros([len(params['im_paths']),params['multiCropEval'],2],dtype=np.int64)\n",
    "        #params['imSizes'] = np.zeros([len(params['im_paths']),params['multiCropEval'],2],dtype=np.int64)\n",
    "        for u in range(len(params['im_paths'])):\n",
    "            height, width = imagesize.get(params['im_paths'][u])\n",
    "            if width < params['input_size'][0]:\n",
    "                height = int(params['input_size'][0]/float(width))*height\n",
    "                width = params['input_size'][0]\n",
    "            if height < params['input_size'][0]:\n",
    "                width = int(params['input_size'][0]/float(height))*width\n",
    "                height = params['input_size'][0]            \n",
    "            ind = 0\n",
    "            for i in range(np.int32(np.sqrt(params['multiCropEval']))):\n",
    "                for j in range(np.int32(np.sqrt(params['multiCropEval']))):\n",
    "                    params['cropPositions'][u,ind,0] = params['input_size'][0]/2+i*((width-params['input_size'][1])/(np.sqrt(params['multiCropEval'])-1))\n",
    "                    params['cropPositions'][u,ind,1] = params['input_size'][1]/2+j*((height-params['input_size'][0])/(np.sqrt(params['multiCropEval'])-1))\n",
    "                    #params['imSizes'][u,ind,0] = curr_im_size[0]\n",
    "\n",
    "                    ind += 1\n",
    "        # Sanity checks\n",
    "        #print(\"Positions\",params['cropPositions'])\n",
    "        # Test image sizes\n",
    "        height = params['input_size'][0]\n",
    "        width = params['input_size'][1]\n",
    "        for u in range(len(params['im_paths'])):\n",
    "            height_test, width_test = imagesize.get(params['im_paths'][u])\n",
    "            if width_test < params['input_size'][0]:\n",
    "                height_test = int(params['input_size'][0]/float(width_test))*height_test\n",
    "                width_test = params['input_size'][0]\n",
    "            if height_test < params['input_size'][0]:\n",
    "                width_test = int(params['input_size'][0]/float(height_test))*width_test\n",
    "                height_test = params['input_size'][0]                \n",
    "            test_im = np.zeros([width_test,height_test]) \n",
    "            for i in range(params['multiCropEval']):\n",
    "                im_crop = test_im[np.int32(params['cropPositions'][u,i,0]-height/2):np.int32(params['cropPositions'][u,i,0]-height/2)+height,np.int32(params['cropPositions'][u,i,1]-width/2):np.int32(params['cropPositions'][u,i,1]-width/2)+width]\n",
    "                if im_crop.shape[0] != params['input_size'][0]:\n",
    "                    print(\"Wrong shape\",im_crop.shape[0],params['im_paths'][u])    \n",
    "                if im_crop.shape[1] != params['input_size'][1]:\n",
    "                    print(\"Wrong shape\",im_crop.shape[1],params['im_paths'][u])      \n",
    "                    \n",
    "    pd.to_pickle(params, 'params.pkl')\n",
    "    \n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "comment_questions,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
